{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Machine Learning","text":""},{"location":"#machine-learning","title":"Machine Learning","text":"Versions 2025.2"},{"location":"classes/concepts/data/","title":"1.3. Data","text":"<p>All the concepts of Machine Learning are based on data. The quality and quantity of available data are fundamental to the success of any machine learning model. In this context, it is important to understand how data is structured, processed, and used to train models.</p>"},{"location":"classes/concepts/data/#nature-of-data","title":"Nature of Data","text":"<p>Data can be thought of as a collection of features or attributes that describe a particular phenomenon or object. In machine learning, data is often represented as a matrix, where each row corresponds to an example and each column corresponds to a feature. This representation is known as the feature space.</p> <p>A feature is a measurable property or characteristic of the phenomenon being studied. Features can be numerical (e.g., height, weight) or categorical (e.g., color, type). The set of features used to describe the data is known as the feature set or feature vector. Features are used to describe the data and can be used as input to machine learning models. Each feature is a dimension of the feature space, and the dataset is represented as a point in this space.</p> <p>Features can be numerical or categorical:</p> <ul> <li>Numerical features are those that can take continuous values, such as height or weight;</li> <li>Categorical features are those that take discrete values, such as color or type.</li> </ul> <p>Depending on the type of machine learning algorithm, features may be treated differently. For example, some algorithms work better with numerical features, while others are more suited for categorical features. In this context, it is necessary to convert categorical features into a format that algorithms can understand, such as using one-hot encoding<sup>1</sup> or label encoding<sup>2</sup>.</p> <p>Additionally, numerical features, such as height or weight, are often normalized to ensure that all features contribute equally to the model. Normalization is a preprocessing technique that adjusts the values of features to a common scale, typically between 0 and 1 or -1 and 1. A common approach to normalization is min-max scaling, which transforms the data by subtracting the minimum value and dividing by the range (maximum - minimum). This ensures that all features are on the same scale and can improve the performance of many machine learning algorithms.</p>"},{"location":"classes/concepts/data/#datasets","title":"Datasets","text":"<p>Data is often stored in datasets, which are structured collections of data that can be easily accessed, managed, and updated. Datasets can be relational (e.g., SQL databases) or non-relational (e.g., NoSQL databases). Relational datasets store data in tables with predefined schemas, while non-relational datasets allow for more flexible data structures. Some common types of datasets used in machine learning include:</p> <ul> <li>UCI Machine Learning Repository: a collection of datasets for machine learning tasks, including classification, regression, and clustering.</li> <li>Kaggle Datasets: a platform that offers a wide variety of datasets for different machine learning tasks, from text classification to image recognition.</li> <li>OpenML: a collaborative platform for sharing and organizing machine learning datasets and experiments.</li> <li>Google Dataset Search: a search engine for datasets across the web, allowing users to find datasets for various machine learning tasks.</li> <li>AWS Open Data Registry: a collection of publicly available datasets hosted on Amazon Web Services, covering a wide range of domains, including climate, healthcare, and transportation.</li> <li>Data.gov: a repository of datasets provided by the U.S. government, covering various topics such as agriculture, health, and energy.</li> <li>FiveThirtyEight Data: a collection of datasets used in articles by FiveThirtyEight, covering topics such as politics, sports, and economics.</li> <li>Awesome Public Datasets: a curated list of high-quality public datasets for various domains.</li> <li>The World Bank Open Data: a collection of global development data, including economic, social, and environmental indicators.</li> <li>IMDB Datasets: a collection of datasets related to movies, TV shows, and actors, useful for natural language processing and recommendation systems.</li> <li>Yelp Open Dataset: a dataset containing business reviews, user data, and check-ins, useful for sentiment analysis and recommendation systems.</li> </ul>"},{"location":"classes/concepts/data/#data-quality","title":"Data Quality","text":"<p>Data quality is a critical aspect of machine learning, as the performance of models heavily depends on the quality of the data used for training. Poor quality data can lead to inaccurate predictions and unreliable models. Common issues with data quality include:</p> <ul> <li>Missing data: values that are not available for some variables;</li> <li>Duplicate data: records that appear more than once in the dataset;</li> <li>Noisy data: values that are inconsistent or incorrect;</li> <li>Imbalanced data: when one class is much more frequent than another, which can lead to a biased model.</li> <li>Inconsistent data: when the data does not follow a consistent pattern or format, making it difficult to analyze and train the model.</li> <li>Irrelevant data: variables that do not contribute to the machine learning task and may harm the model's performance.</li> </ul> <p>To address these issues, it is common to perform a data cleaning and preprocessing process, which may include:</p> <ul> <li>Removing missing data: excluding records with missing values or imputing values based on other observations.</li> <li>Removing duplicates: identifying and removing duplicate records.</li> <li>Handling noisy data: applying smoothing or filtering techniques to reduce noise in the data.</li> <li>Balancing classes: techniques such as undersampling or oversampling - data augmentation<sup>6</sup> - to deal with imbalanced classes.</li> <li>Normalization: adjusting the values of variables to a common scale, ensuring that all variables contribute equally to the model.</li> <li>Transforming variables: applying techniques such as logarithm, square root, or Box-Cox to transform non-linear variables into linear ones.</li> <li>Encoding categorical variables: converting categorical variables into a format that algorithms can understand, such as using one-hot encoding or label encoding.</li> </ul> <p>Additionally, it is important to consider the order of the data, especially in time series problems, where the sequence of the data is crucial for analysis and modeling.</p>"},{"location":"classes/concepts/data/#data-volume-and-balance","title":"Data Volume and Balance","text":"<p>The volume and balance of data are also important factors to consider in machine learning. Data volume refers to the amount of data available for training and testing machine learning models. The larger the volume of data, the more information the model can learn, which usually results in better performance. However, it is also important to consider the quality of the data, as noisy or irrelevant data can harm the model's performance.</p> <p>Additionally, it is important to consider class balancing, especially in classification problems. Class balancing refers to the equitable distribution of classes in the dataset. If one class is much more frequent than another, this can lead to a biased model, which tends to predict the majority class. To address this issue, techniques such as undersampling or oversampling can be used to balance the classes. Undersampling involves removing records from the majority class, while oversampling involves duplicating records from the minority class or generating synthetic data.</p> <p>For supervised learning models, it is essential to have a labeled dataset, where each example has an input (features) and an output (label). This allows the model to learn to map the inputs to the correct outputs.</p> <p>Furthermore, the data can be classified into three main categories:</p> Set Description Train Used to train the model, allowing it to learn the patterns and relationships between features and labels. Test Used to tune the model's hyperparameters and prevent overfitting, ensuring it generalizes well to new examples. Validation Used to evaluate the model's performance on unseen data, ensuring it generalizes well to new examples."},{"location":"classes/concepts/data/#some-examples-of-datasets","title":"Some Examples of Datasets","text":""},{"location":"classes/concepts/data/#salmon-vs-seabass","title":"Salmon vs Seabass","text":"<p>A fictional dataset about salmon and seabass, where each record is labeled as \"salmon\" or \"seabass\". The goal is to better understand how the data can be used to differentiate between the two species. In this context, the features may include, for example: size and brightness<sup>5</sup>.</p>"},{"location":"classes/concepts/data/#problem","title":"Problem","text":"<p>Imagine you have a fish sorting machine. Every day, fishing boats dump tons of fish onto a conveyor belt, and the goal of the machine is to separate the fish, classifying them as \"salmon\" or \"seabass\" based on their characteristics.</p> <p>The conveyor belt has sensors that measure the size and brightness of the fish. Based on these measurements, the machine must decide whether the fish is a salmon or a seabass.</p> \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix} \\] <p>where \\(x_1\\) is the size of the fish and \\(x_2\\) is the brightness of the fish. The machine must learn to classify the fish based on these characteristics, using a function \\(f\\) that maps the input features to the output class: salmon or seabass.</p>"},{"location":"classes/concepts/data/#sample-data","title":"Sample Data","text":"<p>To better understand the data, a sample of fish was taken, where each fish is described by its size and brightness characteristics. The table below presents a sample of the collected data:</p> Size (cm) Brightness (0-10) Species 60 6 salmon 45 5 seabass 78 7 salmon 90 5.2 salmon 71 9 salmon 80 3 seabass 64 6 salmon 58 2 seabass 63 6.8 seabass 50 4 seabass <p>When plotting the data, we can visualize the size and brightness of each fish in a two-dimensional space. Each fish is represented by a point in this space, where the x-axis represents the size and the y-axis represents the brightness. The points are colored according to their species: salmon or seabass.</p> 2025-11-14T11:31:22.661978 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Sample data of salmon and seabass, where each fish is described by its size and brightness characteristics. The points are colored according to their species: salmon (blue) or seabass (orange). For 1-dimensional data, the points are plotted along the x-axis, representing the size and brightness of the fish.</p> 2025-11-14T11:31:22.722203 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Sample data of salmon and seabass, where each fish is described by its size and brightness characteristics. The points are colored according to their species: salmon (blue) or seabass (orange). The x-axis represents the size of the fish, while the y-axis represents its brightness.</p> <p>The machine must learn to draw a line that separates the two classes, salmon and seabass, based on the size and brightness characteristics. This line is called a decision boundary. So that, as soon as a new fish is placed on the conveyor belt, the machine can decide whether it is a salmon or a seabass based on its size and brightness characteristics - as shown in the figure on the right.</p> <p>In general, in the context of classification, the machine must learn to draw decision boundaries in a multidimensional feature space. Allowing, when a new example is presented, the machine to decide which class it belongs to based on the characteristics of the example.</p> <p>Attention</p> <p>The decision boundary is not always linear. In some cases, the data may be distributed in a way that requires a non-linear decision boundary to separate the classes effectively. In such cases, more complex models, such as neural networks or support vector machines with kernels, may be needed to find an appropriate separation.</p>"},{"location":"classes/concepts/data/#iris-dataset","title":"Iris Dataset","text":"<p>UCI Machine Learning Repository: the Iris Dataset is a classic dataset used for classification tasks in machine learning. It was introduced by Sir Ronald A. Fisher in 1936<sup>3</sup> and has since become one of the most widely used datasets in the field<sup>4</sup>.</p> <p>The Iris Dataset is a classic and real dataset used for flower classification. It contains 150 samples of three different species of Iris flowers (Iris setosa, Iris versicolor, and Iris virginica), with four features: petal and sepal length and width.</p> <p></p> <p>The dataset is widely used to demonstrate machine learning algorithms, especially for classification tasks. It is simple enough to be easily understood, but also presents interesting challenges for more complex models.</p> <p>A sample of the Iris dataset is presented in the table below:</p> sepal length(cm) sepal width(cm) petal length(cm) petal width(cm) class 5.7 3.0 4.2 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.2 2.9 4.3 1.3 versicolor 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 6.7 3.0 5.2 2.3 virginica 6.3 2.5 5.0 1.9 virginica 6.5 3.0 5.2 2.0 virginica <p>Sample of the Iris dataset, containing features such as sepal length, sepal width, petal length, and petal width, along with the class of the flower. The dataset is widely used for classification tasks in machine learning.</p> <p>Below there is a code snippet that loads the Iris dataset using the <code>pandas</code> library and visualizes it using <code>matplotlib</code>. The dataset is loaded from a CSV file, and the features are plotted in a scatter plot, with different colors representing the different classes of flowers.</p> <p> </p> Editor (session: default) Run <pre>import pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Carregar o conjunto de dados Iris\niris = load_iris()\n\n# Transforma em DataFrame\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\n)\ndf['class'] = iris.target_names[iris.target]\n\n# Imprime os dados\nprint(df)</pre> Output Clear <pre><code></code></pre> <p></p> <p>Also, the dataset can be visualized using the <code>seaborn</code> library, which provides a high-level interface for drawing attractive statistical graphics:</p> 2025-11-14T11:31:25.646948 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Dataset visualization of the Iris dataset using the <code>seaborn</code> library. The scatter plot shows the relationship between the features of the flowers, with different colors representing the different classes. The diagonal plots show the distribution of each feature, allowing for a better understanding of the data.</p> <p>In this visualization, each feature is represented by an axis, and the flowers are plotted in a multidimensional space. The colors represent the different classes of flowers, allowing for the identification of patterns and separations between the classes. Note that for some configurations, such as petal length vs petal width, the classes are well separated, while in others, such as sepal length vs sepal width, the classes overlap.</p> <p>Real World</p> <p>The Iris dataset is a classic example of a dataset used in machine learning, particularly for classification tasks. It is simple enough to be easily understood, but also presents interesting challenges for more complex models. The dataset is widely used in educational contexts to teach concepts of machine learning and data analysis.</p> <p>One can imagine that in more complex problems, such as image recognition or natural language processing, the data can be much more complex and challenging. Not allowing for a clear visualization of the spatial distribution of features. However, the fundamental principles of machine learning remain the same: understanding the data, properly preprocessing it, and choosing the right model for the task.</p>"},{"location":"classes/concepts/data/#other-datasets","title":"Other Datasets","text":"<p>Data distribution is a crucial aspect of machine learning, as it directly affects the model's ability to learn and generalize. Usually, the nature of the data can be visualized in scatter plots, histograms, or boxplots, allowing for the identification of patterns, trends, and anomalies in the data - of course, when the data has a low number of dimensions (2 or 3).</p> <p>Illustrations of some distributions with only two dimensions are presented below:</p> 2025-11-14T11:31:26.124738 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Data distributions in two dimensions in different spatial formats. For each surface, the separation between classes is made based on the characteristics of the data. The distribution of the data can affect the model's ability to learn and generalize.</p> <p>The figure above presents four different data distributions in two dimensions, each with its own spatial characteristics. The separation between classes is made based on the characteristics of the data, and the distribution of the data can affect the model's ability to learn and generalize. In general, the function of a machine learning technique is to find a separation between classes in order to maximize the model's accuracy.</p>"},{"location":"classes/concepts/data/#summary","title":"Summary","text":"<p>Data is the foundation of any machine learning model. The quality, quantity, and nature of the available data are critical to the model's success. It is important to understand how the data is structured, processed, and used to train models, as well as to consider the volume of data and the balance of classes.</p> <p>In addition, it is essential to perform proper data preprocessing, which may include cleaning, transformation, and normalization, to ensure that models can learn effectively and make accurate predictions.</p> <p>The great challenge in machine learning is to seek the best separation between classes in order to maximize the model's accuracy. This involves not only the choice of algorithm but also a deep understanding of the data and the relationships between variables.</p> <ol> <li> <p>One-Hot Encoding - Wikipedia \u21a9</p> </li> <li> <p>Label Encoding - Scikit-learn \u21a9</p> </li> <li> <p>Fisher, R. A.. 1936. Iris. UCI Machine Learning Repository. https://doi.org/10.24432/C56C76. \u21a9</p> </li> <li> <p>Iris Dataset - Wikipedia \u21a9</p> </li> <li> <p>Richard O. Duda, Peter E. Hart, and David G. Stork. 2000. Pattern Classification (2nd Edition). Wiley-Interscience, USA.\u00a0\u21a9</p> </li> <li> <p>Data Augmentation - Wikipedia \u21a9</p> </li> </ol>"},{"location":"classes/concepts/exercise/","title":"1.4. Exercise","text":""},{"location":"classes/concepts/exercise/#to-do","title":"To Do","text":"<p>Quiz</p> <p>Now, you can practice what you learned in this lesson.</p> <p>Answer the questions in the LMS about the concepts presented here.</p>"},{"location":"classes/concepts/kdd/","title":"1.2. KDD","text":"<p>Knowledge Discovery in Databases (KDD) \u00e9 o processo de identificar padr\u00f5es v\u00e1lidos, \u00fateis e compreens\u00edveis em grandes conjuntos de dados, combinando t\u00e9cnicas de minera\u00e7\u00e3o de dados, estat\u00edstica, aprendizado de m\u00e1quina e gerenciamento de banco de dados. \u00c9 um processo iterativo e interativo, amplamente utilizado em \u00e1reas como ci\u00eancia de dados, intelig\u00eancia de neg\u00f3cios e pesquisa cient\u00edfica. O KDD foi formalizado por Fayyad et al. (1996) e \u00e9 composto por v\u00e1rias etapas, que podem variar ligeiramente dependendo do contexto, mas geralmente seguem a estrutura abaixo. A seguir, as etapas principais:</p>"},{"location":"classes/concepts/kdd/#etapas-do-processo-kdd","title":"Etapas do Processo KDD","text":"<pre><code>flowchart TD\n    A@{ shape: database, label: \"1 - Data Selection\"} --&gt; B@{ shape: procs, label: \"2 - Data Preprocessing\"}\n    B --&gt; C@{ shape: notch-pent, label: \"3 - Data Transformation\"}\n    C --&gt; D@{ shape: subproc, label: \"4 - Data Mining\"}\n    D --&gt; E@{ shape: notch-rect, label: \"5 - Evaluation and Interpretation\"}\n    E --&gt; F@{ shape: doc, label: \"6 - Presentation and Use of Knowledge\"}\n    F --&gt; A\n    F --&gt; D</code></pre>"},{"location":"classes/concepts/kdd/#1-data-selection","title":"1. Data Selection","text":"Descri\u00e7\u00e3o Detalhes Objetivo Identificar e extrair um subconjunto relevante de dados do reposit\u00f3rio (banco de dados, data warehouse, etc.) para an\u00e1lise. Atividades - Compreender o problema de neg\u00f3cio ou a quest\u00e3o de pesquisa.- Definir os atributos (vari\u00e1veis) e inst\u00e2ncias (registros) relevantes.- Filtrar dados com base em crit\u00e9rios espec\u00edficos, como per\u00edodo de tempo, localiza\u00e7\u00e3o ou tipo de cliente. Exemplo Em um banco de dados de vendas, selecionar apenas transa\u00e7\u00f5es de clientes de uma regi\u00e3o espec\u00edfica em um determinado ano. Ferramentas Consultas SQL, ferramentas de ETL (Extract, Transform, Load). Desafios Garantir que os dados selecionados sejam representativos e suficientes para o problema."},{"location":"classes/concepts/kdd/#2-data-preprocessing","title":"2. Data Preprocessing","text":"Descri\u00e7\u00e3o Detalhes Objetivo Preparar os dados brutos para an\u00e1lise, tratando inconsist\u00eancias, ru\u00eddos e dados ausentes. Atividades - Limpeza de dados: Corrigir erros (e.g., valores inconsistentes, duplicatas) e tratar valores ausentes (imputa\u00e7\u00e3o, exclus\u00e3o ou interpola\u00e7\u00e3o).- Integra\u00e7\u00e3o de dados: Combinar dados de fontes heterog\u00eaneas, resolvendo conflitos de formato ou schema.- Redu\u00e7\u00e3o de dados: Eliminar redund\u00e2ncias ou reduzir dimensionalidade (e.g., usando PCA ou sele\u00e7\u00e3o de atributos). Exemplo Substituir valores nulos em uma coluna de idade por uma m\u00e9dia ou mediana, ou normalizar pre\u00e7os para uma mesma moeda. Ferramentas Pandas (Python), R, scripts de limpeza personalizados. Desafios Preservar a integridade dos dados e evitar introdu\u00e7\u00e3o de vi\u00e9s durante a limpeza."},{"location":"classes/concepts/kdd/#3-data-transformation","title":"3. Data Transformation","text":"Descri\u00e7\u00e3o Detalhes Objetivo Converter os dados em um formato adequado para as t\u00e9cnicas de minera\u00e7\u00e3o. Atividades - Normaliza\u00e7\u00e3o (e.g., escalonamento para [0,1]) ou padroniza\u00e7\u00e3o (m\u00e9dia 0, desvio padr\u00e3o 1).- Discretiza\u00e7\u00e3o de vari\u00e1veis cont\u00ednuas (e.g., transformar idades em faixas et\u00e1rias).- Cria\u00e7\u00e3o de novas vari\u00e1veis (feature engineering), como calcular a raz\u00e3o entre duas vari\u00e1veis.- Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas (e.g., one  -hot encoding). Exemplo Transformar uma coluna de datas em vari\u00e1veis como \"dia da semana\" ou \"m\u00eas\". Ferramentas Scikit-learn, SQL, ferramentas de ETL. Desafios Escolher transforma\u00e7\u00f5es que maximizem a performance dos algoritmos de minera\u00e7\u00e3o."},{"location":"classes/concepts/kdd/#4-data-mining","title":"4. Data Mining","text":"Descri\u00e7\u00e3o Detalhes Objetivo Aplicar algoritmos para extrair padr\u00f5es, associa\u00e7\u00f5es, clusters, anomalias ou previs\u00f5es dos dados. Atividades - Escolher a t\u00e9cnica de minera\u00e7\u00e3o apropriada com base no objetivo (classifica\u00e7\u00e3o, regress\u00e3o, clustering, regras de associa\u00e7\u00e3o, etc.).- Executar algoritmos, ajustando hiperpar\u00e2metros e validando resultados.- Exemplos de t\u00e9cnicas: Classifica\u00e7\u00e3o: \u00c1rvores de decis\u00e3o, SVM, redes neurais. Clustering: K-means, DBSCAN. Regras de associa\u00e7\u00e3o: Algoritmo Apriori.  Detec\u00e7\u00e3o de anomalias: Isolation Forest. Exemplo Identificar grupos de clientes com comportamento de compra semelhante usando K-means. Ferramentas Weka, RapidMiner, TensorFlow, PyTorch. Desafios Sele\u00e7\u00e3o do algoritmo certo, overfitting, escalabilidade em grandes datasets."},{"location":"classes/concepts/kdd/#5-pattern-evaluation-and-interpretation","title":"5. Pattern Evaluation and Interpretation","text":"Descri\u00e7\u00e3o Detalhes Objetivo Avaliar a validade, utilidade e novidade dos padr\u00f5es descobertos, interpretando-os no contexto do problema. Atividades - Usar m\u00e9tricas espec\u00edficas para avaliar os padr\u00f5es (e.g., acur\u00e1cia, precis\u00e3o, recall, F1-score para classifica\u00e7\u00e3o; silhouette score para clustering).- Validar os resultados com especialistas do dom\u00ednio ou testes estat\u00edsticos.- Filtrar padr\u00f5es irrelevantes ou redundantes. Exemplo Verificar se um padr\u00e3o como \"clientes que compram X tamb\u00e9m compram Y\" \u00e9 estatisticamente significativo e \u00fatil para estrat\u00e9gias de marketing. Ferramentas Visualiza\u00e7\u00f5es (Matplotlib, Tableau), testes estat\u00edsticos. Desafios Evitar padr\u00f5es esp\u00farios e garantir que os resultados sejam acion\u00e1veis."},{"location":"classes/concepts/kdd/#6-knowledge-presentation-and-use","title":"6. Knowledge Presentation and Use","text":"Descri\u00e7\u00e3o Detalhes Objetivo Comunicar os resultados de forma clara e utiliz\u00e1-los para tomar decis\u00f5es ou resolver problemas. Atividades - Criar relat\u00f3rios, dashboards ou visualiza\u00e7\u00f5es interativas.- Implementar os padr\u00f5es em sistemas operacionais (e.g., recomenda\u00e7\u00f5es em e-commerce).- Documentar o processo para replicabilidade. Exemplo Um dashboard mostrando clusters de clientes para direcionar campanhas de marketing personalizadas. Ferramentas Power BI, Tableau, relat\u00f3rios em LaTeX ou Markdown. Desafios Traduzir resultados t\u00e9cnicos para stakeholders n\u00e3o t\u00e9cnicos."},{"location":"classes/concepts/kdd/#caracteristicas-do-processo-kdd","title":"Caracter\u00edsticas do Processo KDD","text":"<ul> <li>Iterativo: As etapas podem ser revisadas v\u00e1rias vezes (e.g., voltar ao pr\u00e9-processamento ap\u00f3s avaliar padr\u00f5es insatisfat\u00f3rios).</li> <li>Interativo: Envolve colabora\u00e7\u00e3o entre analistas, especialistas do dom\u00ednio e sistemas automatizados.</li> <li>Multidisciplinar: Integra conhecimentos de estat\u00edstica, ci\u00eancia da computa\u00e7\u00e3o, aprendizado de m\u00e1quina e dom\u00ednio do problema.</li> <li>Focado no usu\u00e1rio: O sucesso depende de alinhar os padr\u00f5es descobertos com os objetivos do neg\u00f3cio ou pesquisa.</li> </ul>"},{"location":"classes/concepts/kdd/#consideracoes-adicionais","title":"Considera\u00e7\u00f5es Adicionais","text":"<ul> <li>Desafios Comuns:<ul> <li>Lidar com big data (volume, velocidade, variedade).</li> <li>Garantir privacidade e \u00e9tica no uso de dados (e.g., conformidade com LGPD ou GDPR).</li> <li>Escolher algoritmos que escalem bem e sejam robustos a ru\u00eddos.</li> </ul> </li> <li>Diferen\u00e7a entre KDD e Minera\u00e7\u00e3o de Dados: A minera\u00e7\u00e3o de dados \u00e9 uma etapa espec\u00edfica do KDD, enquanto o KDD abrange o processo completo, desde a sele\u00e7\u00e3o at\u00e9 a aplica\u00e7\u00e3o do conhecimento.</li> <li>Aplica\u00e7\u00f5es: Previs\u00e3o de churn, detec\u00e7\u00e3o de fraudes, recomenda\u00e7\u00e3o de produtos, an\u00e1lise gen\u00f4mica, entre outros.</li> </ul>"},{"location":"classes/concepts/kdd/#resumo","title":"Resumo","text":"<p>O KDD \u00e9 um processo sistem\u00e1tico para transformar dados brutos em conhecimento acion\u00e1vel. Suas etapas (sele\u00e7\u00e3o, pr\u00e9-processamento, transforma\u00e7\u00e3o, minera\u00e7\u00e3o, avalia\u00e7\u00e3o e apresenta\u00e7\u00e3o) s\u00e3o interdependentes e requerem uma combina\u00e7\u00e3o de t\u00e9cnicas computacionais e conhecimento do dom\u00ednio.</p>"},{"location":"classes/concepts/ml/","title":"1.1. Machine Learning","text":"<p>Artificial Intelligence (AI) is a broad field that encompasses various approaches and techniques for creating intelligent systems capable of performing tasks that typically require human intelligence. These tasks include reasoning, learning, perception, and decision-making.</p> <p>AI can be categorized into three main paradigms, each with its own strengths and weaknesses: Symbolic AI, Connectionist AI, and Neuro-Symbolic AI. Each of these paradigms has its own strengths and weaknesses, and they are often used in different contexts depending on the problem being addressed.</p>"},{"location":"classes/concepts/ml/#ai-paradigms","title":"AI Paradigms","text":"Paradigm Description Symbolic AI Focuses on high-level reasoning and knowledge representation using symbols and rules. It excels in tasks requiring logical reasoning, such as theorem proving and expert systems. However, it struggles with perception and learning from raw data. Examples include logic-based systems, expert systems, and knowledge graphs. Connectionist AI Based on artificial neural networks (ANNs), it excels in pattern recognition, learning from large datasets, and handling noisy data. It is particularly effective in tasks like image and speech recognition. However, it often lacks interpretability and struggles with reasoning tasks. Examples include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers. Neuro-Symbolic AI Combines the strengths of both symbolic and connectionist AI, aiming to create systems that can reason about complex problems while also learning from data. It leverages symbolic reasoning capabilities alongside neural networks to enhance interpretability and reasoning abilities. Examples include neuro-symbolic systems that integrate symbolic logic with neural networks, such as knowledge-augmented language models and graph neural networks. 2025-11-14T11:31:26.539076 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Neuro-Symbolic AI combines symbolic reasoning with neural networks, leveraging the strengths of both approaches. It aims to create systems that can reason about complex problems while also learning from data.</p> <p>This approach is particularly useful in tasks that require both high-level reasoning and the ability to learn from raw data, such as natural language understanding and complex decision-making.</p> <p>There are several approaches to implementing AI. Machine learning (ML) is one of the most common methods, where algorithms learn from data to make predictions or decisions. Neural networks, a subset of ML, are inspired by the structure and function of the human brain and are particularly effective in tasks like image and speech recognition. Deep learning, a more advanced form of neural networks, uses multiple layers of processing to extract complex patterns from large datasets.</p> 2025-11-14T11:31:26.643389 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Artificial Neural Networks (ANNs), or simply neural networks, are computational models inspired by the structure and function of biological neural networks. They consist of interconnected nodes (neurons) that process information in a manner similar to the way neurons in the human brain operate. ANNs are capable of learning from data, making them powerful tools for various tasks such as image recognition, natural language processing, and decision-making.</p> <p>Neural networks are the backbone of many modern AI applications, enabling machines to learn from experience and improve their performance over time. They are particularly effective in handling complex patterns and large datasets, making them suitable for a wide range of applications, from computer vision to speech recognition.</p>"},{"location":"classes/concepts/ml/#milestones","title":"Milestones","text":"<p>Foundations of Neural Networks</p> 1943<p> Laid the theoretical groundwork for ANNs, inspiring future computational models of the brain. Warren McCulloch and Walter Pitts publish a paper introducing the first mathematical model of a neural network, describing neurons as logical decision-making units. McCulloch, &amp; W. S., Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. </p> <p>Turing Test Proposed</p> 1950<p> Established a benchmark for assessing AI capabilities, influencing the philosophical and practical development of AI. Alan Turing publishes \"Computing Machinery and Intelligence,\" proposing the Turing Test to evaluate a machine's ability to exhibit intelligent behavior indistinguishable from that of a human. Turing, A. M. (1950). Computing Machinery and Intelligence. </p> <p>Birth of AI as a Discipline</p> 1956<p> Marked the formal establishment of AI as a field of study, fostering research into machine intelligence. The Dartmouth Conference, organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, coins the term artificial intelligence. McCarthy, J., Minsky, M., Rochester, N., Shannon, C. (1955). Dartmouth Conference Proposal. </p> <p>Perceptron Introduced</p> 1958<p> Pioneered the concept of a simple neural network, laying the foundation for future developments in machine learning and neural networks. Frank Rosenblatt develops the Perceptron, an early artificial neural network capable of learning to classify patterns. It consists of a single layer of output nodes connected to input features, using a step function to produce binary outputs. Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. </p> <p>Limitations of Perceptrons \u2192 AI Winter</p> 1969<p> Highlighted the limitations of early neural networks, leading to a temporary decline in interest in neural networks and AI. Marvin Minsky and Seymour Papert publish \"Perceptrons,\" critiquing the limitations of single-layer perceptrons, particularly their inability to solve non-linearly separable problems like the XOR problem. This work leads to a decline in neural network research for over a decade. Led to the first \"AI winter,\" a period of reduced funding and interest in neural networks, shifting focus to symbolic AI. Minsky, M., Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. </p> <p>Backpropagation Rediscovered</p> 1986<p> Revived interest in ANNs by overcoming limitations of single-layer perceptrons, paving the way for deep learning. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams publish a paper on backpropagation, enabling training of multi-layer neural networks. Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. </p> <p>Universal Approximation Theorem</p> 1989<p> Established the theoretical foundation for neural networks' ability to approximate any continuous function, leading to the development of deep learning. George Cybenko proves that a feedforward neural network with a single hidden layer can approximate any continuous function on compact subsets of \\(R^n\\) under mild conditions on the activation function. Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. </p> <p>Deep Blue Defeats Chess Champion</p> 1997<p> Showcased the potential of AI in strategic games, leading to advancements in game-playing AI and deep learning. IBM's Deep Blue defeats world chess champion Garry Kasparov in a six-game match, marking a significant milestone in AI's ability to compete with human intelligence in complex tasks. Campbell, M., Hoane, A. J., &amp; Hsu, F. (2002). Deep Blue. </p> <p>Convolutional Neural Networks (CNNs)</p> 1998<p> Revolutionized computer vision and image processing, enabling breakthroughs in object recognition and classification. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton publish a paper on CNNs, introducing the LeNet architecture for handwritten digit recognition. LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. </p> <p>Deep Learning Renaissance</p> 2006<p> Sparked the modern deep learning era by showing that deep networks could be trained efficiently. Geoffrey Hinton and colleagues introduce deep belief networks, demonstrating effective pre-training for deep neural networks. Hinton, G. E., Osindero, S., &amp; Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. </p> <p>AlexNet and the ImageNet Breakthrough</p> 2012<p> Demonstrated the superiority of deep learning in computer vision, leading to widespread adoption. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton\u2019s AlexNet wins the ImageNet competition, achieving unprecedented accuracy in image classification using deep CNNs. Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. </p> <p>Generative Adversarial Networks (GANs)</p> 2014<p> Introduced a novel approach to generative modeling, enabling the creation of realistic synthetic data. Ian Goodfellow and colleagues introduce GANs, a framework for training generative models using adversarial training. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). Generative adversarial nets. </p> <p>DeepMind\u2019s AlphaGo</p> 2015<p> Showcased deep learning\u2019s ability to tackle complex strategic games, advancing AI research. DeepMind\u2019s AlphaGo, using deep reinforcement learning and neural networks, defeats professional Go player Lee Sedol. Silver, D., et al. (2016). Mastering the game of Go with deep neural networks and tree search. </p> <p>Transformers and Attention Mechanisms</p> 2017<p> Revolutionized natural language processing and sequence modeling, enabling breakthroughs in machine translation and text generation. Ashish Vaswani and colleagues introduce the Transformer architecture, which uses self-attention mechanisms to process sequences in parallel, significantly improving performance in NLP tasks. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., &amp; Polosukhin, I. (2017). Attention is all you need.  </p> <p>BERT and Pre-trained Language Models</p> 2018<p> Set new standards in NLP by introducing pre-training and fine-tuning techniques, enabling models to understand context and semantics better. Jacob Devlin and colleagues introduce BERT (Bidirectional Encoder Representations from Transformers), a pre-trained language model that achieves state-of-the-art results on various NLP benchmarks. Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. </p> : <p>GPT-3 and Large Language Models</p> 2020<p> Showcased the capabilities of large-scale language models, enabling advancements in natural language understanding and generation. OpenAI releases GPT-3, a 175 billion parameter language model, demonstrating impressive performance in various NLP tasks, including text generation, translation, and question answering. Brown, T. B., Mann, B., Ryder, N., Subbiah, S., Kaplan, J., Dhariwal, P., Neelakantan, S., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, D., Litwin, M., Gray, S., Chess, B., Clark, J., Berridge, S., Zaremba, W., &amp; Amodei, D. (2020). Language models are few-shot learners. </p> <p>DALL-E and Image Generation</p> 2021<p> Enabled the generation of high-quality images from textual descriptions, showcasing the potential of multimodal AI. OpenAI introduces DALL-E, a model capable of generating images from textual descriptions, demonstrating the power of combining language and vision. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., &amp; Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. </p>"},{"location":"classes/concepts/ml/#machine-learning","title":"Machine Learning","text":"<p>In the context of AI, machine learning (ML) techniques are used to enable systems to learn from data and improve their performance over time without being explicitly programmed. These techniques allow AI systems to adapt and generalize from examples, making them capable of handling a wide range of tasks, from image recognition to natural language processing.</p> <p>The techniques are often split into two main categories: supervised learning and unsupervised learning.</p> <p>Supervised Learning</p> <p>Supervised learning involves training a model on labeled data, where the input data is paired with the correct output. This allows the model to learn patterns and make predictions based on new, unseen data.</p> <p>This approach is particularly effective when there is a clear relationship between the input features and the output labels, allowing the model to generalize from the training data to make accurate predictions on new data. Examples include classification tasks (e.g., identifying objects in images) and regression tasks (e.g., predicting house prices based on features).</p> <p>Unsupervised Learning</p> <p>Unsupervised learning, on the other hand, involves training a model on unlabeled data, where the model must find patterns and relationships within the data without explicit guidance.</p> <p>This approach is useful for discovering hidden structures in data, such as clusters or groups, without prior knowledge of the labels. It is often used in exploratory data analysis and feature extraction. Examples include clustering tasks (e.g., grouping similar documents) and dimensionality reduction tasks (e.g., reducing the number of features in a dataset while preserving important information).</p> <p>There are also semi-supervised learning techniques, which combine both labeled and unlabeled data to improve model performance. This approach is particularly useful when labeled data is scarce or expensive to obtain, allowing the model to leverage the abundance of unlabeled data to enhance its learning.</p> <p>Also, there are reinforcement learning techniques, where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. This approach is particularly effective for tasks that involve sequential decision-making, such as game playing or robotic control.</p> <p>Machine learning techniques address a wide range of problems, primarily through classification and regression, which are core supervised learning tasks. Classification involves predicting discrete labels or categories based on input features, while regression focuses on predicting continuous values. These approaches are extensively applied across domains such as image recognition, natural language processing, and time series forecasting. However, machine learning also includes other techniques like clustering, dimensionality reduction, reinforcement learning, and anomaly detection, expanding its applicability to diverse challenges.</p> <p>Few examples of machine learning techniques include:</p> Technique Description Decision Trees A tree-like model used for classification and regression tasks, where each internal node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome. Random Forest An ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting. It works by training multiple decision trees on different subsets of the data and averaging their predictions. Support Vector Machines (SVM) A supervised learning algorithm that finds the optimal hyperplane to separate different classes in the feature space. It is effective for high-dimensional data and can handle both linear and non-linear classification tasks. K-Nearest Neighbors (KNN) A simple algorithm that classifies new instances based on the majority class of their k-nearest neighbors in the feature space. It is a non-parametric method that can be used for both classification and regression tasks. Naive Bayes A probabilistic classifier based on Bayes' theorem, assuming independence between features. It is particularly effective for text classification tasks, such as spam detection and sentiment analysis. Linear Regression A statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. It is commonly used for predicting continuous outcomes based on input features. Logistic Regression A statistical method used for binary classification tasks, where the output is a probability that can be mapped to two classes. It models the relationship between input features and the log-odds of the outcome using a logistic function. K-Means Clustering An unsupervised learning algorithm that partitions data into k clusters based on feature similarity. It iteratively assigns data points to the nearest cluster centroid and updates the centroids until convergence. Principal Component Analysis (PCA) A dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving important features. It identifies the principal components that capture the most variance in the data, making it useful for visualization and feature extraction. Gradient Boosting An ensemble learning technique that builds a series of weak learners (usually decision trees) in a sequential manner, where each new learner corrects the errors of the previous ones. It is effective for both classification and regression tasks and is widely used in machine learning competitions."},{"location":"classes/concepts/ml/#neural-networks","title":"Neural Networks","text":"<p>Neural networks are a class of machine learning models inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) organized in layers, where each connection has an associated weight that is adjusted during training. Neural networks are particularly effective for tasks involving complex patterns, such as image and speech recognition.</p> <p>Neural networks can be categorized into several types, including: </p> <ul> <li>Feedforward Neural Networks (FNNs): The simplest type of neural network where information flows in one direction, from input to output, without cycles. They are commonly used for tasks like classification and regression.</li> <li>Convolutional Neural Networks (CNNs): Specialized neural networks designed for processing grid-like data, such as images. They use convolutional layers to automatically learn spatial hierarchies of features, making them highly effective for image recognition tasks.</li> <li>Recurrent Neural Networks (RNNs): Neural networks designed for sequential data, such as time series or natural language. They have connections that loop back on themselves, allowing them to maintain a memory of previous inputs. This makes them suitable for tasks like language modeling and speech recognition.</li> <li>Transformers: A type of neural network architecture that uses self-attention mechanisms to process sequences of data. They have revolutionized natural language processing tasks, enabling models like BERT and GPT to achieve state-of-the-art performance in various language understanding tasks.</li> </ul>"},{"location":"classes/concepts/ml/#deep-learning","title":"Deep Learning","text":"<p>Deep learning is a subset of machine learning that focuses on using deep neural networks with many layers to learn complex representations of data. It has achieved remarkable success in various domains, including computer vision, natural language processing, and speech recognition. Deep learning models are capable of automatically learning hierarchical features from raw data, eliminating the need for manual feature engineering. This has led to significant advancements in AI applications, enabling systems to perform tasks that were previously considered challenging or impossible.</p>"},{"location":"classes/concepts/ml/#additional-resources","title":"Additional Resources","text":"<ol> <li> <p>Wiki - Neuro-Symbolic AI \u21a9</p> </li> <li> <p>2020, Forbes - Symbolism Versus Connectionism In AI: Is There A Third Way? \u21a9</p> </li> <li> <p>Garcez, A.d., Lamb, L.C. Neurosymbolic AI: the 3rd wave. Artif Intell Rev 56, 12387\u201312406 (2023). doi.org/10.1007/s10462-023-10448-w \u21a9</p> </li> <li> <p>Hodgkin\u2013Huxley model. Alan Hodgkin and Andrew Huxley develop a mathematical model of the action potential in neurons, describing how neurons transmit signals through electrical impulses. This model is foundational for understanding neural dynamics and influences the development of artificial neural networks. Hodgkin, A. L., Huxley, A. F. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve. .\u00a0\u21a9</p> </li> <li> <p>Visual Cortex and Monocular Deprivation. David H. Hubel and Torsten N. Wiesel conduct pioneering research on the visual cortex of cats, demonstrating how visual experience shapes neural development. Their work on monocular deprivation shows that depriving one eye of visual input during a critical period leads to permanent changes in the visual cortex, highlighting the importance of experience in neural plasticity. Hubel, D. H., &amp; Wiesel, T. N. (1963). Effects of monocular deprivation in kittens. .\u00a0\u21a9</p> </li> <li> <p>Neocognitron. Kunihiko Fukushima develops the Neocognitron, an early convolutional neural network (CNN) model that mimics the hierarchical structure of the visual cortex. This model is a precursor to modern CNNs and demonstrates the potential of hierarchical feature extraction in image recognition tasks. Fukushima, K. (1980). Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position. .\u00a0\u21a9</p> </li> <li> <p>Hopfield Networks. John Hopfield introduces Hopfield networks, a type of recurrent neural network that can serve as associative memory systems. These networks are capable of storing and recalling patterns, laying the groundwork for later developments in neural network architectures. Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. .\u00a0\u21a9</p> </li> <li> <p>Self-Organizing Maps (SOM). Teuvo Kohonen develops Self-Organizing Maps, a type of unsupervised learning algorithm that maps high-dimensional data onto a lower-dimensional grid. SOMs are used for clustering and visualization of complex data, providing insights into the structure of the data. Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. .\u00a0\u21a9</p> </li> <li> <p>Long Short-Term Memory (LSTM) Networks. Sepp Hochreiter and J\u00fcrgen Schmidhuber introduce LSTM networks, a type of recurrent neural network designed to learn long-term dependencies in sequential data. This architecture addresses the vanishing gradient problem in RNNs, enabling effective modeling of long-term dependencies in sequential data. Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. \".\u00a0\u21a9</p> </li> <li> <p>Residual Networks (ResNets). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun introduce Residual Networks (ResNets), a deep learning architecture that uses skip connections to allow gradients to flow more easily through deep networks. This architecture enables the training of very deep neural networks, significantly improving performance on image recognition tasks. He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Deep residual learning for image recognition. \u21a9</p> </li> </ol>"},{"location":"classes/decision_tree/","title":"6. Decision Tree","text":"<p>As \u00e1rvores de decis\u00e3o s\u00e3o uma t\u00e9cnica popular de aprendizado de m\u00e1quina supervisionado usada para classifica\u00e7\u00e3o e regress\u00e3o. Elas representam decis\u00f5es e suas poss\u00edveis consequ\u00eancias em uma estrutura hier\u00e1rquica, facilitando a interpreta\u00e7\u00e3o dos resultados.</p> <pre><code>graph TD;\n    T1((Teste 1)) --&gt;|Sim| T2((Teste 2))\n    T1 --&gt;|N\u00e3o| T3((Teste 3))\n    T2 --&gt;|Sim| R1[Resultado 1]\n    T2 --&gt;|N\u00e3o| R2[Resultado 2]\n    T3 --&gt;|Sim| R3[Resultado 3]\n    T3 --&gt;|N\u00e3o| R4[Resultado 4]</code></pre> Exemplo: ir para praia ou n\u00e3o? <p>Ap\u00f3s dias de anota\u00e7\u00f5es sobre o comportamento de uma pessoa, foi poss\u00edvel criar uma tabela com os seguintes registros:</p> Dia Sol? Vento? Praia? 1 Sim Sim N\u00e3o 2 Sim Sim N\u00e3o 3 Sim N\u00e3o Sim 4 N\u00e3o N\u00e3o N\u00e3o 5 N\u00e3o Sim N\u00e3o 6 N\u00e3o N\u00e3o N\u00e3o <p>A partir desses dados, podemos construir uma \u00e1rvore de decis\u00e3o para prever se a pessoa ir\u00e1 \u00e0 praia com base nas condi\u00e7\u00f5es clim\u00e1ticas.</p> <pre><code>graph TD;\n    A((Sol?)) --&gt;|Sim| B((Vento?))\n    A --&gt;|N\u00e3o| C[N\u00e3o ir \u00e0 praia]\n    B --&gt;|Sim| D[N\u00e3o ir \u00e0 praia]\n    B --&gt;|N\u00e3o| E[Ir \u00e0 praia]</code></pre> <p>\u00c1rvore de decis\u00e3o simples para prever se a pessoa ir\u00e1 \u00e0 praia com base nas condi\u00e7\u00f5es clim\u00e1ticas. A partir da pergunta \"Sol?\", a \u00e1rvore se divide em dois caminhos: se h\u00e1 sol, verifica-se se h\u00e1 vento. Se n\u00e3o h\u00e1 sol, a decis\u00e3o \u00e9 n\u00e3o ir \u00e0 praia. Se n\u00e3o h\u00e1 vento, a decis\u00e3o \u00e9 ir \u00e0 praia. Fonte: Didatica Tech - \u00c1rvores de Decis\u00e3o.</p>"},{"location":"classes/decision_tree/#consideracoes","title":"Considera\u00e7\u00f5es","text":"<p>Vantagens</p> <ul> <li>Interpreta\u00e7\u00e3o f\u00e1cil: A estrutura em \u00e1rvore facilita a visualiza\u00e7\u00e3o e compreens\u00e3o das decis\u00f5es tomadas pelo modelo.</li> <li>N\u00e3o requer normaliza\u00e7\u00e3o: \u00c1rvores de decis\u00e3o n\u00e3o s\u00e3o sens\u00edveis \u00e0 escala dos dados, o que significa que n\u00e3o \u00e9 necess\u00e1rio normalizar ou padronizar as vari\u00e1veis.</li> <li>Capacidade de lidar com dados categ\u00f3ricos e num\u00e9ricos: Elas podem trabalhar com ambos os tipos de dados sem necessidade de transforma\u00e7\u00e3o pr\u00e9via.</li> </ul> <p>Desvantagens</p> <ul> <li>Tend\u00eancia ao overfitting: \u00c1rvores de decis\u00e3o podem se ajustar demais aos dados de treinamento, capturando ru\u00eddos e padr\u00f5es irrelevantes.</li> <li>Instabilidade: Pequenas varia\u00e7\u00f5es nos dados podem resultar em \u00e1rvores completamente diferentes, tornando o modelo menos robusto.</li> </ul>"},{"location":"classes/decision_tree/#nomenclatura","title":"Nomenclatura","text":"<p>As \u00e1rvores de decis\u00e3o s\u00e3o compostas por n\u00f3s (representando testes em atributos) e folhas (representando resultados ou classes finais). O processo de constru\u00e7\u00e3o da \u00e1rvore envolve a sele\u00e7\u00e3o do atributo mais informativo para dividir os dados em subconjuntos, minimizando a impureza (e.g., usando medidas como entropia ou \u00edndice Gini)<sup>1</sup>.</p> <p></p> <p>Estrutura de uma \u00c1rvore de Decis\u00e3o: os n\u00f3s representam testes em atributos, enquanto as folhas representam os resultados finais. Fonte: Aulas - \u00c1rvores.</p> <p>O objetivo de uma \u00e1rvore de decis\u00e3o \u00e9 criar uma estrutura que minimize a impureza dos n\u00f3s, resultando em folhas que contenham exemplos da mesma classe ou com valores semelhantes. Isso \u00e9 feito atrav\u00e9s de um processo iterativo de divis\u00e3o dos dados, onde em cada n\u00f3 \u00e9 escolhido o atributo que melhor separa os dados em termos de classe ou valor.</p> <p>Existem algumas m\u00e9tricas comuns usadas para medir a qualidade de uma divis\u00e3o, incluindo:</p> <ul> <li>\u00cdndice Gini: Mede a impureza dos dados, onde um valor de 0 indica pureza total (todos os exemplos pertencem \u00e0 mesma classe).</li> <li>Entropia: Mede a incerteza ou aleatoriedade dos dados, onde uma entropia de 0 indica que todos os exemplos pertencem \u00e0 mesma classe.</li> <li>Ganho de Informa\u00e7\u00e3o: Mede a redu\u00e7\u00e3o da entropia ap\u00f3s a divis\u00e3o dos dados.</li> <li>Redu\u00e7\u00e3o da Vari\u00e2ncia: Usada em \u00e1rvores de decis\u00e3o para regress\u00e3o, mede a redu\u00e7\u00e3o da vari\u00e2ncia dos valores ap\u00f3s a divis\u00e3o dos dados.</li> <li>Chi-quadrado: Usado para medir a independ\u00eancia entre vari\u00e1veis categ\u00f3ricas, ajudando a identificar intera\u00e7\u00f5es significativas entre atributos.</li> </ul> <p>Para o c\u00e1lculo do coeficiente de Gini (mais usado em \u00e1rvores de decis\u00e3o):</p> \\[ g_i = 1 - \\sum_{i=1}^{n} p_i^2 \\] <p>onde \\( p_i \\) \u00e9 a propor\u00e7\u00e3o de cada classe \\( i \\) no conjunto de dados.</p> <p>Mais baixo a impureza, mais puro o n\u00f3.</p>"},{"location":"classes/decision_tree/#construcao","title":"Constru\u00e7\u00e3o","text":"<p>Neste exemplo, temos um conjunto de dados sobre transa\u00e7\u00f5es financeiras, onde cada transa\u00e7\u00e3o \u00e9 classificada como \"Fraude\" ou \"Normal\". A \u00e1rvore de decis\u00e3o pode ser usada para prever se uma nova transa\u00e7\u00e3o \u00e9 fraudulenta ou n\u00e3o, com base em caracter\u00edsticas como o valor da transa\u00e7\u00e3o e o per\u00edodo.</p> <p>Fraude</p> data sample (20/38)plot Valor Periodo Classe 2500 Diurno Normal 1500 Diurno Normal 700 Diurno Normal 3700 Diurno Normal 5600 Diurno Normal 8000 Diurno Normal 3200 Noturno Normal 2900 Noturno Normal 5950 Diurno Normal 630 Noturno Fraude 1800 Diurno Normal 2700 Diurno Normal 3300 Diurno Normal 4200 Noturno Normal 7000 Diurno Normal 500 Diurno Normal 850 Noturno Normal 900 Diurno Normal 4700 Diurno Fraude 5650 Diurno Fraude 2025-11-14T11:31:27.046544 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Para construir a \u00e1rvore de decis\u00e3o, os dados s\u00e3o divididos em n\u00f3s com base nas caracter\u00edsticas mais informativas, minimizando a impureza dos n\u00f3s. Para construir a \u00e1rvore, o algoritmo avalia cada atributo e escolhe aquele que melhor separa as classes, utilizando m\u00e9tricas como o \u00edndice Gini ou entropia.</p>"},{"location":"classes/decision_tree/#passo-a-passo","title":"Passo a passo","text":"<ol> <li>Definir o n\u00f3 com os dados daquele ramo.</li> <li>Calcular a impureza de cada atributo.</li> <li>Escolher o atributo que melhor separa os dados.</li> <li>Dividir os dados com base no atributo escolhido.</li> <li>Repetir o processo para cada subconjunto at\u00e9 que um crit\u00e9rio de parada seja atendido (e.g., todos os exemplos em um n\u00f3 pertencem \u00e0 mesma classe ou um n\u00famero m\u00ednimo de exemplos \u00e9 atingido).</li> </ol> <p>Para definir o n\u00f3 raiz, o algoritmo avalia todos os atributos e calcula a impureza de cada um. O atributo com a menor impureza \u00e9 escolhido como o n\u00f3 raiz. Em seguida, os dados s\u00e3o divididos com base nesse atributo, criando ramos na \u00e1rvore. O processo \u00e9 repetido recursivamente para cada ramo at\u00e9 que todos os n\u00f3s sejam folhas (ou seja, n\u00e3o possam ser divididos mais).</p> Feature Sim N\u00e3o Valor &gt;= 3000 18 20 Fraude 5 1 Normal 13 19 Periodo = Noturno 14 24 Fraude 4 2 Normal 10 22 <p>C\u00e1lculo do \u00edndice de Gini para cada crit\u00e9rio sobre atributos:</p> \\[ \\text{Gini}(\\text{Crit\u00e9rio}) = 1 - \\left(\\frac{fraude}{fraude + normal}\\right)^2 - \\left(\\frac{normal}{fraude + normal}\\right)^2  \\] Valor &gt;= 3000Periodo = Noturno \\[ \\text{Gini}(\\text{Valor}\\geq 3000) = 1 - \\left(\\frac{5}{18}\\right)^2 - \\left(\\frac{13}{18}\\right)^2 = 0.4012 \\] \\[ \\text{Gini}(\\text{Valor} &lt; 3000) = 1 - \\left(\\frac{1}{20}\\right)^2 - \\left(\\frac{19}{20}\\right)^2 = 0.0950 \\] <p>Normalizando os valores, temos:</p> \\[ \\text{Pureza do n\u00f3} = \\frac{\\text{18}}{38} \\cdot 0.4012 + \\frac{20}{38} \\cdot 0.0950 = 0.2401 \\] \\[ \\text{Gini}(\\text{Periodo} = \\text{Noturno}) = 1 - \\left(\\frac{4}{14}\\right)^2 - \\left(\\frac{10}{14}\\right)^2 = 0.4082 \\] \\[ \\text{Gini}(\\text{Periodo} \\neq \\text{Noturno}) = 1 - \\left(\\frac{2}{24}\\right)^2 - \\left(\\frac{22}{24}\\right)^2 = 0.1528 \\] <p>Normalizando os valores, temos:</p> \\[ \\text{Pureza do n\u00f3} = \\frac{\\text{14}}{38} \\cdot 0.4082 + \\frac{24}{38} \\cdot 0.1528 = 0.2469 \\] <p>\u00c1rvore de decis\u00e3o resultante pode ser representada da seguinte forma:</p> \u00c1rvore constru\u00eddadecision treecode <pre><code>graph TD;\n    A{Valor &gt;= 3000?} --&gt;|Sim| B{Periodo = Noturno?}\n    A --&gt;|N\u00e3o| C[Normal]\n    B --&gt;|Sim| D[Fraude]\n    B --&gt;|N\u00e3o| E[Normal]</code></pre> <p>Accuracy: 0.62  2025-11-14T11:31:27.293644 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\nplt.figure(figsize=(12, 10))\n\ndf = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/fraude.csv')\n\nlabel_encoder = LabelEncoder()\n\n# Carregar o conjunto de dados\nx = df[['Valor', 'Periodo']]\nx['Periodo'] = label_encoder.fit_transform(x['Periodo'])\ny = df['Classe']\n\n# Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\n# Avaliar o modelo\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\ntree.plot_tree(classifier)\n\n# Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n\nplt.close()\n</code></pre>"},{"location":"classes/decision_tree/#implementacao-com-bibliotecas","title":"Implementa\u00e7\u00e3o com Bibliotecas","text":"<p>As \u00e1rvores de decis\u00e3o podem ser implementadas usando bibliotecas populares como <code>scikit-learn</code> em Python, que oferece uma interface simples para criar e treinar modelos de \u00e1rvores de decis\u00e3o. A seguir \u00e9 um exemplo b\u00e1sico de como criar uma \u00e1rvore de decis\u00e3o para classifica\u00e7\u00e3o:</p> <p>Iris Dataset</p> outputdatasetcode <p>Validation Accuracy: 1.0000 Feature Importances: </p> Feature Importance 3 petal width (cm) 0.557323 2 petal length (cm) 0.404457 1 sepal width (cm) 0.019110 0 sepal length (cm) 0.019110 2025-11-14T11:31:27.510865 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ sepal_l sepal_w petal_l petal_w class 4.4 2.9 1.4 0.2 setosa 5.8 4 1.2 0.2 setosa 6.4 2.9 4.3 1.3 versicolor 5.4 3.9 1.7 0.4 setosa 5.7 4.4 1.5 0.4 setosa 6.3 2.5 4.9 1.5 versicolor 6.8 2.8 4.8 1.4 versicolor 7.2 3.6 6.1 2.5 virginica 6.1 2.8 4.7 1.2 versicolor 5 3.3 1.4 0.2 setosa 5 2.3 3.3 1 versicolor 5.1 3.8 1.6 0.2 setosa 5.1 3.4 1.5 0.2 setosa 7.3 2.9 6.3 1.8 virginica 6.8 3.2 5.9 2.3 virginica <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\n\nplt.figure(figsize=(12, 10))\n\n# Carregar o conjunto de dados Iris\niris = load_iris()\nx = iris.data\ny = iris.target\n\n# Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n# Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\n# Evaluate the model\ny_pred = classifier.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\n\n# Optional: Print feature importances\nfeature_importance = pd.DataFrame({\n    'Feature': iris.feature_names,\n    'Importance': classifier.feature_importances_\n})\nprint(\"&lt;br&gt;Feature Importances:\")\nprint(feature_importance.sort_values(by='Importance', ascending=False).to_html())\n\ntree.plot_tree(classifier)\n\n# Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n\nplt.close()\n</code></pre> <p>Titanic Dataset</p> decision treedatasetcode <p>Accuracy: 0.78  2025-11-14T11:31:28.176045 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 1 0 3 Braund, Mr. Owen Harris male 22 1 0 A/5 21171 7.25 nan S 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 PC 17599 71.2833 C85 C 3 1 3 Heikkinen, Miss. Laina female 26 0 0 STON/O2. 3101282 7.925 nan S 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 113803 53.1 C123 S 5 0 3 Allen, Mr. William Henry male 35 0 0 373450 8.05 nan S <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n# Preprocess the data\ndef preprocess(df):\n    # Fill missing values\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n\n    # Convert categorical variables\n    label_encoder = LabelEncoder()\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])\n\n    # Select features\n    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n    return df[features]\n\nplt.figure(figsize=(12, 10))\n\ndf = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv')\n\n# Carregar o conjunto de dados\nx = preprocess(df)\ny = df['Survived']\n\n# Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\n# Avaliar o modelo\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\ntree.plot_tree(classifier)\n\n# Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n\nplt.close()\n</code></pre>"},{"location":"classes/decision_tree/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p> 29.ago 9:00</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize o algoritmo de \u00e1rvore de decis\u00e3o para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio \u00c1rvore de Decis\u00e3o. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 20 2 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 10 3 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 20 4 Treinamento do Modelo Implementa\u00e7\u00e3o do modelo Decision Tree. 10 5 Avalia\u00e7\u00e3o do Modelo Avalia\u00e7\u00e3o do desempenho do modelo utilizando m\u00e9tricas apropriadas. 20 6 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. Obrigat\u00f3rio: uso do template-projeto-integrador, individual. 20"},{"location":"classes/decision_tree/#adicional","title":"Adicional","text":"<ol> <li> <p>Aulas - \u00c1rvores \u21a9</p> </li> <li> <p>Didatica Tech - \u00c1rvores de Decis\u00e3o \u21a9</p> </li> </ol>"},{"location":"classes/decision_tree/fraude_tree/","title":"Fraude tree","text":"<pre><code>graph TD;\n    A{Valor &gt;= 3000?} --&gt;|Sim| B{Periodo = Noturno?}\n    A --&gt;|N\u00e3o| C[Normal]\n    B --&gt;|Sim| D[Fraude]\n    B --&gt;|N\u00e3o| E[Normal]</code></pre>"},{"location":"classes/gradient_boosting/","title":"8. Gradient boosting","text":"<p>Gradient Boosting is a powerful machine learning technique used for regression and classification tasks. It builds an ensemble of weak learners, typically decision trees, in a sequential manner where each new tree attempts to correct the errors made by the previous ones. This method is known for its high predictive accuracy and ability to handle various types of data.</p> <ol> <li> <p>XGBoost Tutorial - Introduction to Boosted Trees\uf0c1 \u21a9</p> </li> </ol>"},{"location":"classes/kmeans/","title":"4. K-Means","text":"<p>K-Means Clustering is an unsupervised machine learning algorithm used to partition a dataset into \\( K \\) distinct, non-overlapping clusters. The algorithm assigns each data point to the cluster with the nearest centroid (mean) based on a distance metric, typically Euclidean distance. It is widely used in data analysis, pattern recognition, and image processing due to its simplicity and efficiency.</p>"},{"location":"classes/kmeans/#key-concepts","title":"Key Concepts","text":"<ul> <li>Clusters: Groups of data points that are similar to each other based on a distance metric.</li> <li>Unsupervised Learning: The algorithm works without labeled data, identifying patterns based solely on the data's structure.</li> <li>Centroids: These are the \"centers\" of the clusters, represented as the mean (average) of all points in a cluster.</li> <li>K: The number of clusters, which must be specified in advance (e.g., K=3 means dividing data into 3 clusters).</li> <li>Distance Metric: Typically, Euclidean distance is used to measure how far a data point is from a centroid. The goal is to assign points to the nearest centroid.</li> <li> <p>Objective: Minimize the within-cluster sum of squares (WCSS), which is the sum of squared distances between each point and its assigned centroid. Mathematically, for a dataset \\( X = \\{x_1, x_2, \\dots, x_n\\} \\) and centroids \\( \\mu = \\{\\mu_1, \\mu_2, \\dots, \\mu_K\\} \\), the objective is:</p> \\[ \\arg\\min_{\\mu} \\sum_{i=1}^K \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 \\] <p>where \\( C_i \\) is the set of points in cluster \\( i \\).</p> </li> </ul> <p>K-Means assumes clusters are spherical and equally sized, which may not always hold for real data.</p>"},{"location":"classes/kmeans/#algorithm-step-by-step","title":"Algorithm: Step-by-Step","text":"<p>The K-Means algorithm is iterative and consists of the following steps:</p> <ol> <li> <p>Initialization:</p> <ul> <li>Choose the value of K (number of clusters).</li> <li>Randomly select K initial centroids from the dataset. (A common improvement is K-Means++ initialization, which spreads out the initial centroids to avoid poor starting points.)</li> </ul> </li> <li> <p>Assignment Step (Expectation):</p> <ul> <li>For each data point in the dataset, calculate its distance to all K centroids.</li> <li>Assign the point to the cluster with the closest centroid (using Euclidean distance or another metric).</li> <li>This creates K clusters, where each point belongs to exactly one cluster.</li> </ul> </li> <li> <p>Update Step (Maximization):</p> <ul> <li>For each cluster, recalculate the centroid as the mean (average) of all points assigned to that cluster.</li> <li>Update the centroids with these new values.</li> </ul> </li> <li> <p>Iteration:</p> <ul> <li>Repeat steps 2 and 3 until one of the stopping criteria is met:<ul> <li>Centroids no longer change (or change by less than a small threshold, e.g., 0.001).</li> <li>A maximum number of iterations is reached (to prevent infinite loops).</li> <li>The WCSS decreases minimally between iterations.</li> </ul> </li> </ul> </li> <li> <p>Output:</p> <ul> <li>The final centroids and the cluster assignments for each data point.</li> </ul> </li> </ol> <p>The algorithm converges because the WCSS is non-increasing with each iteration, but it may converge to a local optimum (not always the global best). Running it multiple times with different initializations helps mitigate this.</p>"},{"location":"classes/kmeans/#example-1","title":"Example 1","text":"<p>Suppose you have a 2D dataset with 5 points: (1,2), (2,1), (5,8), (6,7), (8,6). Let K=2.</p> <ul> <li>Initialization: Randomly pick centroids, say C1=(1,2) and C2=(5,8).</li> <li>Assignment:<ul> <li>(1,2) and (2,1) are closer to C1 \u2192 Cluster 1.</li> <li>(5,8), (6,7), (8,6) are closer to C2 \u2192 Cluster 2.</li> </ul> </li> <li>Update:<ul> <li>New C1 = average of (1,2) and (2,1) = (1.5, 1.5).</li> <li>New C2 = average of (5,8), (6,7), (8,6) = (6.33, 7).</li> </ul> </li> <li>Repeat: Reassign points based on new centroids. This continues until stable.</li> </ul> <p>After convergence, you might end up with two clusters: one around (1.5,1.5) and one around (6.33,7).</p>"},{"location":"classes/kmeans/#example-2","title":"Example 2","text":"ResultCode 2025-11-14T11:31:31.924316 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.cluster import KMeans\n\nplt.figure(figsize=(12, 10))\n\n# Generate sample data\nnp.random.seed(42)\nX = np.concatenate([\n    np.random.normal(0, 1, (100, 2)),\n    np.random.normal(5, 1, (100, 2)),\n    np.random.normal(10, 1, (100, 2))\n])\n\n# Run K-Means\nkmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100, random_state=42)\nlabels = kmeans.fit_predict(X)\n\n# Plot results\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           c='red', marker='*', s=200, label='Centroids')\nplt.title('K-Means Clustering Results')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\n\n# # Print centroids and inertia\n# print(\"Final centroids:\", kmeans.cluster_centers_)\n# print(\"Inertia (WCSS):\", kmeans.inertia_)\n\n# # Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n\nplt.close()\n</code></pre>"},{"location":"classes/kmeans/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":"Aspect Advantages Disadvantages Simplicity Easy to understand and implement; computationally efficient (O(n) per iteration). Sensitive to initial centroid placement; may converge to local optima. Scalability Works well on large datasets with linear time complexity. Assumes spherical clusters; struggles with non-convex or varying densities. Output Produces tight, compact clusters; interpretable centroids. Requires predefined K; outliers can skew results."},{"location":"classes/kmeans/#choosing-k","title":"Choosing K","text":"<ul> <li>Elbow Method: Plot WCSS vs. K and look for the \"elbow\" where the rate of decrease slows (e.g., K=3 if the curve bends sharply there).</li> <li>Silhouette Score: Measures how similar points are within their cluster vs. other clusters (higher is better, range -1 to 1).</li> <li>Other methods: Gap statistic or domain knowledge.</li> </ul>"},{"location":"classes/kmeans/#implementation-tip","title":"Implementation Tip","text":"<p>In Python, you can use scikit-learn's <code>KMeans</code> class: </p><pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\nX = np.array([[1,2], [2,1], [5,8], [6,7], [8,6]])  # Your data\nkmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\nkmeans.fit(X)\nprint(kmeans.labels_)  # Cluster assignments: e.g., [0, 0, 1, 1, 1]\nprint(kmeans.cluster_centers_)  # Centroids\n</code></pre><p></p> <p>K-Means is a foundational algorithm, but variants like hierarchical clustering or DBSCAN may be better for certain data types. If you have specific data or code to run, let me know for a demo!</p>"},{"location":"classes/kmeans/#additional","title":"Additional","text":""},{"location":"classes/kmeans/#k-means-initialization-explanation","title":"K-Means++ Initialization: Explanation","text":"<p>K-Means++ is an improved initialization method for the K-Means clustering algorithm, designed to address the sensitivity of standard K-Means to the initial placement of centroids. Randomly choosing initial centroids in standard K-Means can lead to poor clustering results or convergence to suboptimal local minima. K-Means++ mitigates this by strategically selecting initial centroids to be spread out across the data, improving both the quality of clusters and convergence speed.</p>"},{"location":"classes/kmeans/#why-k-means","title":"Why K-Means++?","text":"<p>In standard K-Means, centroids are often initialized randomly, which can result in:</p> <ul> <li>Poor clustering: Random centroids might be too close to each other, leading to unbalanced or suboptimal clusters.</li> <li>Slow convergence: Bad initial placements require more iterations to reach a stable solution.</li> <li>Inconsistent results: Different runs produce varying clusters due to random initialization.</li> </ul> <p>K-Means++ addresses these issues by choosing initial centroids in a way that maximizes their separation, reducing the likelihood of poor starting conditions.</p>"},{"location":"classes/kmeans/#k-means-initialization","title":"K-Means++ Initialization","text":"<p>The K-Means++ algorithm selects the initial K centroids iteratively, using a probabilistic approach that favors points farther from already chosen centroids. Here\u2019s the step-by-step process for a dataset \\( X = \\{x_1, x_2, \\dots, x_n\\} \\) and \\( K \\) clusters:</p> <ol> <li> <p>First Centroid:</p> <ul> <li>Randomly select one data point from the dataset as the first centroid \\( \\mu_1 \\). This is typically done uniformly at random to ensure fairness.</li> </ul> </li> <li> <p>Subsequent Centroids:</p> <ul> <li>For each remaining centroid (from 2 to K):<ul> <li>Compute the squared Euclidean distance \\( D(x) \\) from each data point \\( x \\) to the nearest already-selected centroid.</li> <li>Assign a probability to each point \\( x \\): \\( \\frac{D(x)^2}{\\sum_{x' \\in X} D(x')^2} \\). Points farther from existing centroids have a higher probability of being chosen.</li> <li>Select the next centroid by sampling a point from the dataset, weighted by these probabilities.</li> </ul> </li> <li>This ensures new centroids are likely to be far from existing ones, spreading them across the data.</li> </ul> </li> <li> <p>Repeat:</p> <ul> <li>Continue selecting centroids until all K are chosen.</li> </ul> </li> <li> <p>Proceed to K-Means:</p> <ul> <li>Use these K centroids as the starting point for the standard K-Means algorithm (assign points to nearest centroids, update centroids, iterate until convergence).</li> </ul> </li> </ol>"},{"location":"classes/kmeans/#mathematical-intuition","title":"Mathematical Intuition","text":"<p>The probability function \\( \\frac{D(x)^2}{\\sum D(x')^2} \\) uses squared distances to emphasize points that are farther away. This creates a \"repulsive\" effect, where new centroids are more likely to be placed in regions of the dataset that are not yet covered by existing centroids. The result is a set of initial centroids that are well-distributed, reducing the chance of clustering points into suboptimal groups.</p> <p>The expected approximation ratio of K-Means++ is \\( O(\\log K) \\)-competitive with the optimal clustering, a significant improvement over random initialization, which has no such guarantee.</p>"},{"location":"classes/kmeans/#example","title":"Example","text":"<p>Suppose you have a dataset with points: (1,1), (2,2), (8,8), (9,9), and you want \\( K=2 \\):</p> <ul> <li>Step 1: Randomly pick (1,1) as the first centroid.</li> <li> <p>Step 2: Calculate squared distances to (1,1):</p> <ul> <li>(1,1): \\( 0^2 = 0 \\)</li> <li>(2,2): \\( (1^2 + 1^2) = 2 \\)</li> <li>(8,8): \\( (7^2 + 7^2) = 98 \\)</li> <li>(9,9): \\( (8^2 + 8^2) = 128 \\)</li> <li>Total: \\( 0 + 2 + 98 + 128 = 228 \\).</li> <li> <p>Probabilities:</p> <p>(1,1): \\( 0/228 = 0 \\),</p> <p>(2,2): \\( 2/228 \\approx 0.009 \\),</p> <p>(8,8): \\( 98/228 \\approx 0.43 \\),</p> <p>(9,9): \\( 128/228 \\approx 0.56 \\).</p> </li> <li> <p>Likely pick (9,9) or (8,8) as the second centroid due to their high probabilities (far from (1,1)).</p> </li> </ul> </li> <li> <p>Result: Centroids like (1,1) and (9,9) are well-spread, leading to better clustering than if (1,1) and (2,2) were chosen.</p> </li> </ul>"},{"location":"classes/kmeans/#advantages-and-disadvantages_1","title":"Advantages and Disadvantages","text":"Aspect Advantages Disadvantages Quality Produces better initial centroids, leading to lower WCSS and better clusters. Slightly more computationally expensive than random initialization. Convergence Often converges faster due to better starting points (fewer iterations). Still requires predefined K; sensitive to outliers (can skew distances). Consistency More consistent results across runs compared to random initialization. Random first centroid can still introduce some variability."},{"location":"classes/kmeans/#computational-cost","title":"Computational Cost","text":"<ul> <li>Random Initialization: O(K) for picking K random points.</li> <li>K-Means++: O(nK) for computing distances to select K centroids, where n is the number of points. This is a small overhead compared to the K-Means iterations (O(nKI), where I is the number of iterations), and the improved clustering quality often outweighs the cost.</li> </ul>"},{"location":"classes/kmeans/#implementation","title":"Implementation","text":"<p>In Python\u2019s scikit-learn, K-Means++ is the default initialization method for the <code>KMeans</code> class: </p><pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\nX = np.array([[1,1], [2,2], [8,8], [9,9]])\nkmeans = KMeans(n_clusters=2, init='k-means++', random_state=42, n_init=10)\nkmeans.fit(X)\nprint(kmeans.labels_)  # Cluster assignments\nprint(kmeans.cluster_centers_)  # Centroids\n</code></pre> The <code>init='k-means++'</code> parameter explicitly sets K-Means++ initialization (though it\u2019s default in scikit-learn).<p></p>"},{"location":"classes/kmeans/#practical-notes","title":"Practical Notes","text":"<ul> <li>Choosing K: K-Means++ still requires you to specify K. Use methods like the elbow method or silhouette score to determine an optimal K.</li> <li>Outliers: Outliers can disproportionately affect centroid selection due to squared distances. Preprocessing (e.g., removing outliers) can help.</li> <li>Scalability: For very large datasets, variants like scalable K-Means++ or mini-batch K-Means can be used to reduce computational cost.</li> </ul> <p>K-Means++ is a robust improvement over random initialization, widely used in practice due to its balance of simplicity and effectiveness. If you have a dataset or want a visual demo of K-Means++ vs. random initialization, let me know!</p>"},{"location":"classes/kmeans/#additional_1","title":"Additional","text":""},{"location":"classes/kmeans/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p> 21.sep 23:59</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize o algoritmo de K-Means para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio K-Means. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 20 2 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 10 3 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 20 4 Treinamento do Modelo Implementa\u00e7\u00e3o do modelo KNN. 10 5 Avalia\u00e7\u00e3o do Modelo Avalia\u00e7\u00e3o do desempenho do modelo utilizando m\u00e9tricas apropriadas. 20 6 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. Obrigat\u00f3rio: uso do template-projeto-integrador, individual. 20"},{"location":"classes/knn/","title":"3. KNN","text":"<p>K-Nearest Neighbors (KNN) is a simple, versatile, and non-parametric machine learning algorithm used for classification and regression tasks. It operates on the principle of similarity, predicting the label or value of a data point based on the majority class or average of its k nearest neighbors in the feature space. KNN is intuitive and effective for small datasets or when interpretability is key.</p>"},{"location":"classes/knn/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p>Instance-Based Learning: KNN is a lazy learning algorithm, meaning no explicit training phase is required. It stores the entire dataset and performs calculations at prediction time.</p> </li> <li> <p>Distance Metric: The algorithm measures the distance between data points to identify the nearest neighbors. Common metrics include:</p> Metric Formula Euclidean distance \\( \\displaystyle \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} \\) Manhattan distance \\( \\displaystyle \\sum_{i=1}^n \\|x_i - y_i\\| \\) Minkowski distance \\( \\displaystyle \\left( \\sum_{i=1}^n \\|x_i - y_i\\|^p \\right)^{1/p} \\) </li> <li> <p>K Value: The number of neighbors considered. A small k can be sensitive to noise, while a large k smooths predictions but may dilute patterns.</p> </li> <li> <p>Decision Rule:</p> <ul> <li>Classification: The majority class among the k neighbors determines the predicted class.</li> <li>Regression: The average (or weighted average) of the k neighbors' values is used.</li> </ul> </li> </ul>"},{"location":"classes/knn/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>KNN relies on distance calculations to find neighbors. For a data point \\( x \\), the algorithm: 1. Computes the distance to all points in the dataset using a chosen metric (e.g., Euclidean distance). 2. Selects the k closest points. 3. For classification, assigns the class with the most votes among the k neighbors. For regression, computes the mean of their values.</p>"},{"location":"classes/knn/#example-classification","title":"Example: Classification","text":"<p>Given a dataset \\( D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\} \\), where \\( x_i \\) is a feature vector and \\( y_i \\) is the class label, predict the class of a new point \\( x \\):</p> <ul> <li>Calculate distances \\( d(x, x_i) \\) for all \\( i \\).</li> <li>Sort distances and select the k smallest.</li> <li>Count the class labels of these k points and assign the majority class to \\( x \\).</li> </ul>"},{"location":"classes/knn/#weighted-knn","title":"Weighted KNN","text":"<p>In weighted KNN, neighbors contribute to the prediction based on their distance. Closer neighbors have higher influence, often weighted by the inverse of their distance:</p> \\[ w_i = \\frac{1}{d(x, x_i)} \\] <p>For regression, the prediction is:</p> \\[ \\hat{y} = \\frac{\\sum_{i=1}^k w_i y_i}{\\sum_{i=1}^k w_i} \\]"},{"location":"classes/knn/#visualizing-knn","title":"Visualizing KNN","text":"<p>To illustrate, consider a 2D dataset with two classes (blue circles and red triangles). For a new point (green star), KNN identifies the k nearest points and assigns the majority class.</p> <p></p> <p>Figure: KNN with k=3. The green star is classified based on the majority class (blue circles) among its three nearest neighbors.</p> <p>For regression, imagine predicting a continuous value (e.g., house price) based on the average of the k nearest houses\u2019 prices.</p>"},{"location":"classes/knn/#plot-decision-boundary","title":"Plot: Decision Boundary","text":"<p>KNN\u2019s decision boundary is non-linear and depends on the data distribution. Below is an example of decision boundaries for different k values:</p> <p></p> <p>Figure: Decision boundaries for k=1, k=5, and k=15. Smaller k leads to more complex boundaries, while larger k smooths them.</p>"},{"location":"classes/knn/#pros-and-cons-of-knn","title":"Pros and Cons of KNN","text":""},{"location":"classes/knn/#pros","title":"Pros","text":"<ul> <li>Simplicity: Easy to understand and implement.</li> <li>Non-Parametric: Makes no assumptions about data distribution, suitable for non-linear data.</li> <li>Versatility: Works for both classification and regression.</li> <li>Adaptability: Can handle multi-class problems and varying data types with appropriate distance metrics.</li> </ul>"},{"location":"classes/knn/#cons","title":"Cons","text":"<ul> <li>Computational Cost: Slow for large datasets, as it requires calculating distances for every prediction.</li> <li>Memory Intensive: Stores the entire dataset, which can be problematic for big data.</li> <li>Sensitive to Noise: Outliers or irrelevant features can degrade performance.</li> <li>Curse of Dimensionality: Performance drops in high-dimensional spaces due to sparse data.</li> <li>Choosing K: Requires careful tuning of k and distance metric to balance bias and variance.</li> </ul>"},{"location":"classes/knn/#knn-implementation","title":"KNN Implementation","text":"<p>Below are two implementations of KNN: one from scratch and one using Python\u2019s scikit-learn library.</p>"},{"location":"classes/knn/#from-scratch","title":"From Scratch","text":"<p>This implementation includes a basic KNN classifier using Euclidean distance.</p> ResultCode <p>Accuracy: 1.00 </p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass KNNClassifier:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        predictions = [self._predict(x) for x in X]\n        return np.array(predictions)\n\n    def _predict(self, x):\n        # Compute Euclidean distances\n        distances = [np.sqrt(np.sum((x - x_train)**2)) for x_train in self.X_train]\n        # Get indices of k-nearest neighbors\n        k_indices = np.argsort(distances)[:self.k]\n        # Get corresponding labels\n        k_nearest_labels = [self.y_train[i] for i in k_indices]\n        # Return majority class\n        most_common = max(set(k_nearest_labels), key=k_nearest_labels.count)\n        return most_common\n\n# Example usage\n\n# Generate synthetic dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train and predict\nknn = KNNClassifier(k=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n</code></pre>"},{"location":"classes/knn/#using-scikit-learn","title":"Using Scikit-Learn","text":"ResultCode <p>Accuracy: 1.00  2025-11-14T11:31:32.187410 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\n\nplt.figure(figsize=(12, 10))\n\n# Generate synthetic dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train KNN model\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n\n# Visualize decision boundary\nh = 0.02  # Step size in mesh\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.3)\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, style=y, palette=\"deep\", s=100)\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"KNN Decision Boundary (k=3)\")\n\n# Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n\nplt.close()\n</code></pre>"},{"location":"classes/knn/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p> 16.sep 23:59</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize o algoritmo de KNN para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio KNN. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 20 2 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 10 3 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 20 4 Treinamento do Modelo Implementa\u00e7\u00e3o do modelo KNN. 10 5 Avalia\u00e7\u00e3o do Modelo Avalia\u00e7\u00e3o do desempenho do modelo utilizando m\u00e9tricas apropriadas. 20 6 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. Obrigat\u00f3rio: uso do template-projeto-integrador, individual. 20"},{"location":"classes/metrics/","title":"Index","text":""},{"location":"classes/metrics/#considerations-for-neural-networks","title":"Considerations for Neural Networks","text":"<ul> <li>Classification: Metrics like log loss and AUC-ROC are particularly relevant for neural networks, as they align with probabilistic outputs (e.g., softmax) and gradient-based optimization. For imbalanced datasets, F1-score or AUC-PR are preferred over accuracy.</li> <li>Regression: MSE and RMSE are commonly used as loss functions in neural networks, but MAE or Huber loss may be chosen for robustness to outliers. R\u00b2 is useful for post-training evaluation but not typically as a training objective.</li> <li>Domain-Specific Nuances: In multi-class or multi-label classification (e.g., in CNNs for image tasks), metrics like macro/micro-averaged F1-scores are used. For time-series regression with RNNs, metrics like RMSE or MAPE are adapted to temporal dependencies.</li> </ul>"},{"location":"classes/metrics/#summary","title":"Summary","text":"<p>Selecting the appropriate metric depends on the task, dataset characteristics (e.g., imbalance, outliers), and application requirements. For classification, precision, recall, and F1-score are critical for imbalanced data, while AUC-ROC provides a threshold-agnostic evaluation. For regression, RMSE and MAE are standard, with MAPE useful for relative errors. These metrics, implemented in libraries like scikit-learn or TensorFlow, guide model evaluation and optimization in neural network development.</p> <p>Below is a detailed list of metrics commonly used to evaluate the accuracy and performance of classification and regression models in machine learning, including neural networks. The metrics are categorized based on their applicability to classification or regression tasks, with explanations of their purpose and mathematical formulations where relevant.</p>"},{"location":"classes/metrics/#classification-metrics","title":"Classification Metrics","text":"<p>Classification tasks involve predicting discrete class labels. The following metrics assess the accuracy and effectiveness of such models:</p> <ol> <li>Accuracy</li> <li>Purpose: Measures the proportion of correct predictions across all classes.</li> <li>Formula: \\( \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} = \\frac{TP + TN}{TP + TN + FP + FN} \\)<ul> <li>\\( TP \\): True Positives, \\( TN \\): True Negatives, \\( FP \\): False Positives, \\( FN \\): False Negatives.</li> </ul> </li> <li> <p>Use Case: Suitable for balanced datasets but misleading for imbalanced ones.</p> </li> <li> <p>Precision</p> </li> <li>Purpose: Evaluates the proportion of positive predictions that are actually correct.</li> <li>Formula: \\( \\text{Precision} = \\frac{TP}{TP + FP} \\)</li> <li> <p>Use Case: Important when false positives are costly (e.g., spam detection).</p> </li> <li> <p>Recall (Sensitivity or True Positive Rate)</p> </li> <li>Purpose: Measures the proportion of actual positives correctly identified.</li> <li>Formula: \\( \\text{Recall} = \\frac{TP}{TP + FN} \\)</li> <li> <p>Use Case: Critical when false negatives are costly (e.g., disease detection).</p> </li> <li> <p>F1-Score</p> </li> <li>Purpose: Harmonic mean of precision and recall, balancing both metrics.</li> <li>Formula: \\( \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\)</li> <li> <p>Use Case: Useful for imbalanced datasets where both precision and recall matter.</p> </li> <li> <p>Area Under the ROC Curve (AUC-ROC)</p> </li> <li>Purpose: Measures the model\u2019s ability to distinguish between classes across all thresholds.</li> <li>Formula: Area under the curve plotting True Positive Rate (Recall) vs. False Positive Rate (\\( \\frac{FP}{FP + TN} \\)).</li> <li> <p>Use Case: Effective for binary classification and assessing model robustness.</p> </li> <li> <p>Area Under the Precision-Recall Curve (AUC-PR)</p> </li> <li>Purpose: Focuses on precision and recall trade-off, especially for imbalanced datasets.</li> <li>Formula: Area under the curve plotting Precision vs. Recall.</li> <li> <p>Use Case: Preferred when positive class is rare (e.g., fraud detection).</p> </li> <li> <p>Confusion Matrix</p> </li> <li>Purpose: Provides a tabular summary of prediction outcomes (TP, TN, FP, FN).</li> <li> <p>Use Case: Offers detailed insights into class-specific performance, especially for multi-class problems.</p> </li> <li> <p>Log Loss (Logarithmic Loss or Cross-Entropy Loss)</p> </li> <li>Purpose: Penalizes incorrect predictions based on predicted probabilities.</li> <li>Formula: \\( \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] \\)<ul> <li>\\( y_i \\): True label, \\( \\hat{y}_i \\): Predicted probability.</li> </ul> </li> <li> <p>Use Case: Common in probabilistic classifiers like neural networks with softmax outputs.</p> </li> <li> <p>Matthews Correlation Coefficient (MCC)</p> </li> <li>Purpose: Balances all four confusion matrix quadrants, robust for imbalanced data.</li> <li>Formula: \\( \\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} \\)</li> <li> <p>Use Case: Preferred for a single, comprehensive metric in binary classification.</p> </li> <li> <p>Cohen\u2019s Kappa</p> <ul> <li>Purpose: Measures agreement between predicted and true labels, adjusted for chance.</li> <li>Formula: \\( \\kappa = \\frac{p_o - p_e}{1 - p_e} \\)</li> <li>\\( p_o \\): Observed agreement, \\( p_e \\): Expected agreement by chance.</li> <li>Use Case: Useful for multi-class problems or when chance agreement is a concern.</li> </ul> </li> </ol>"},{"location":"classes/metrics/#regression-metrics","title":"Regression Metrics","text":"<p>Regression tasks predict continuous values. The following metrics evaluate the accuracy of predicted values against true values:</p> <ol> <li>Mean Absolute Error (MAE)</li> <li>Purpose: Measures the average absolute difference between predictions and true values.</li> <li>Formula: \\( \\text{MAE} = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i| \\)<ul> <li>\\( y_i \\): True value, \\( \\hat{y}_i \\): Predicted value, \\( N \\): Number of samples.</li> </ul> </li> <li> <p>Use Case: Robust to outliers, interpretable as average error.</p> </li> <li> <p>Mean Squared Error (MSE)</p> </li> <li>Purpose: Measures the average squared difference between predictions and true values.</li> <li>Formula: \\( \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\)</li> <li> <p>Use Case: Sensitive to outliers, commonly used in neural network loss functions.</p> </li> <li> <p>Root Mean Squared Error (RMSE)</p> </li> <li>Purpose: Square root of MSE, providing error in the same units as the target.</li> <li>Formula: \\( \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2} \\)</li> <li> <p>Use Case: Preferred for interpretable error magnitude, widely used in forecasting.</p> </li> <li> <p>Mean Absolute Percentage Error (MAPE)</p> </li> <li>Purpose: Measures average percentage error relative to true values.</li> <li>Formula: \\( \\text{MAPE} = \\frac{1}{N} \\sum_{i=1}^N \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\cdot 100 \\)</li> <li> <p>Use Case: Useful when relative errors matter (e.g., financial predictions), but sensitive to zero or near-zero true values.</p> </li> <li> <p>R-Squared (Coefficient of Determination)</p> </li> <li>Purpose: Measures the proportion of variance in the dependent variable explained by the model.</li> <li>Formula: \\( R^2 = 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2} \\)<ul> <li>\\( \\bar{y} \\): Mean of true values.</li> </ul> </li> <li> <p>Use Case: Indicates model fit, with values closer to 1 indicating better fit.</p> </li> <li> <p>Adjusted R-Squared</p> </li> <li>Purpose: Adjusts R\u00b2 for the number of predictors, penalizing overly complex models.</li> <li>Formula: \\( \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(N - 1)}{N - k - 1} \\right) \\)<ul> <li>\\( k \\): Number of predictors.</li> </ul> </li> <li> <p>Use Case: Useful when comparing models with different numbers of features.</p> </li> <li> <p>Median Absolute Error</p> </li> <li>Purpose: Measures the median of absolute differences, highly robust to outliers.</li> <li>Formula: \\( \\text{MedAE} = \\text{median}(|y_1 - \\hat{y}_1|, \\dots, |y_N - \\hat{y}_N|) \\)</li> <li> <p>Use Case: Preferred in datasets with extreme values or non-Gaussian errors.</p> </li> <li> <p>Huber Loss</p> </li> <li>Purpose: Combines MSE and MAE, less sensitive to outliers than MSE.</li> <li>Formula:       [      L_\\delta(y_i, \\hat{y}_i) =       \\begin{cases}       \\frac{1}{2}(y_i - \\hat{y}_i)^2 &amp; \\text{if } |y_i - \\hat{y}_i| \\leq \\delta \\      \\delta |y_i - \\hat{y}_i| - \\frac{1}{2}\\delta^2 &amp; \\text{otherwise}      \\end{cases}      ]</li> <li>Use Case: Used in robust regression tasks, often as a loss function in neural networks.</li> </ol>"},{"location":"classes/metrics/#considerations-for-neural-networks_1","title":"Considerations for Neural Networks","text":"<ul> <li>Classification: Metrics like log loss and AUC-ROC are particularly relevant for neural networks, as they align with probabilistic outputs (e.g., softmax) and gradient-based optimization. For imbalanced datasets, F1-score or AUC-PR are preferred over accuracy.</li> <li>Regression: MSE and RMSE are commonly used as loss functions in neural networks, but MAE or Huber loss may be chosen for robustness to outliers. R\u00b2 is useful for post-training evaluation but not typically as a training objective.</li> <li>Domain-Specific Nuances: In multi-class or multi-label classification (e.g., in CNNs for image tasks), metrics like macro/micro-averaged F1-scores are used. For time-series regression with RNNs, metrics like RMSE or MAPE are adapted to temporal dependencies.</li> </ul>"},{"location":"classes/metrics/#conclusion","title":"Conclusion","text":"<p>Selecting the appropriate metric depends on the task, dataset characteristics (e.g., imbalance, outliers), and application requirements. For classification, precision, recall, and F1-score are critical for imbalanced data, while AUC-ROC provides a threshold-agnostic evaluation. For regression, RMSE and MAE are standard, with MAPE useful for relative errors. These metrics, implemented in libraries like scikit-learn or TensorFlow, guide model evaluation and optimization in neural network development.</p>"},{"location":"classes/metrics/model-evaluation/","title":"Model evaluation","text":"<p>ANN model evaluation involves measuring performance using metrics like accuracy, precision, recall, and Root Mean Square Error (RMSE) for regression tasks. It also requires tools such as a confusion matrix to identify error types and an ROC curve for classification thresholds, alongside techniques like cross-validation and analyzing the loss function to ensure robustness and generalizability.  Key Evaluation Metrics: Accuracy: The ratio of correct predictions to the total number of predictions, useful for classification tasks.  Precision: Measures the proportion of true positives among all predicted positives.  Recall: Measures the proportion of true positives among all actual positives.  Root Mean Square Error (RMSE): A statistical indicator for regression tasks, measuring the mean difference between predicted and actual values; lower RMSE values indicate better performance.  Loss Function: A measure of the error between the model's predicted output and the actual output, which the model aims to minimize.  Key Evaluation Techniques: Confusion Matrix: A table that categorizes the number of true positives, true negatives, false positives, and false negatives to assess a classifier's performance.  Cross-Validation: A technique where the model's performance is tested on different, separate validation datasets to ensure its ability to generalize to new data.  Receiver Operating Characteristic (ROC) Curve: A plot that illustrates the relationship between the true positive rate and the false positive rate at various classification thresholds.  Data Split: Dividing the dataset into training, validation, and testing sets to train the model, tune hyperparameters, and evaluate its final performance, respectively.  Hyperparameter Tuning: Evaluating the impact of different settings for hyperparameters (e.g., the number of neurons in a layer, learning rate) on the model's accuracy and time constraints.  Considerations: Data Imbalance: Accuracy can be misleading in datasets where some classes are more frequent than others.  Overfitting: ANNs have a high memory and can sometimes overfit the training data, meaning they perform well on past data but poorly on new, different data.  Model Interpretability: Understanding why a model makes a certain prediction is crucial and can be achieved through techniques like feature importance or Layer-wise Relevance Propagation. </p>"},{"location":"classes/metrics/classification/","title":"5.1. Classification","text":"<p>Below is a detailed list of metrics commonly used to evaluate the accuracy and performance of classification and regression models in machine learning, including neural networks. The metrics are categorized based on their applicability to classification or regression tasks, with explanations of their purpose and mathematical formulations where relevant.</p>"},{"location":"classes/metrics/classification/#classification-metrics","title":"Classification Metrics","text":"<p>Classification tasks involve predicting discrete class labels. The following metrics assess the accuracy and effectiveness of such models:</p> Metric Purpose Use Case Accuracy \\( \\displaystyle \\frac{TP + TN}{TP + TN + FP + FN} \\) Measures the proportion of correct predictions across all classes Suitable for balanced datasets but misleading for imbalanced ones Precision \\( \\displaystyle \\frac{TP}{TP + FP} \\) Evaluates the proportion of positive predictions that are actually correct Important when false positives are costly (e.g., spam detection) Recall (Sensitivity) \\( \\displaystyle \\frac{TP}{TP + FN} \\) Assesses the proportion of actual positives correctly identified Critical when false negatives are costly (e.g., disease detection) F1-Score \\( \\displaystyle 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\) Harmonic mean of precision and recall, balancing both metrics Useful for imbalanced datasets where both precision and recall matter AUC-ROC  Area under the curve plotting True Positive Rate (Recall) vs. False Positive Rate \\( \\displaystyle \\left( \\frac{FP}{FP + TN} \\right) \\) Measures the model\u2019s ability to distinguish between classes across all thresholds Effective for binary classification and assessing model robustness AUC-PR  Area under the curve plotting Precision vs. Recall Focuses on precision and recall trade-off, especially for imbalanced datasets Preferred when positive class is rare (e.g., fraud detection) Confusion Matrix<sup>1</sup> Provides a tabular summary of prediction outcomes (TP, TN, FP, FN) Offers detailed insights into class-specific performance, especially for multi-class problems Hamming Loss \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{L} \\sum_{j=1}^L \\mathbf{1}(y_{ij} \\neq \\hat{y}_{ij}) \\) Calculates the fraction of incorrect labels to the total number of labels Suitable for multi-label classification tasks Balanced Accuracy \\( \\displaystyle \\frac{1}{C} \\sum_{i=1}^C \\frac{TP_i}{TP_i + FN_i} \\) Average of recall obtained on each class, useful for imbalanced datasets Effective for multi-class problems with class imbalance"},{"location":"classes/metrics/classification/#loss-functions","title":"Loss Functions","text":"<p>Loss functions commonly used in classification tasks:</p> Metric Purpose Use Case Cross-Entropy Loss \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\) Measures the performance of a classification model whose output is a probability value between 0 and 1. It increases as the predicted probability diverges from the actual label. Commonly used in classification tasks with probabilistic outputs. Binary Cross-Entropy \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\) Used for binary classification tasks, measuring the difference between two probability distributions. Commonly used in binary classification problems. Categorical Cross-Entropy \\( \\displaystyle -\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) \\) Used when there are two or more label classes. It is a generalization of binary cross-entropy to multi-class problems. Suitable for multi-class classification tasks with one-hot encoded labels. Sparse Categorical Cross-Entropy \\( \\displaystyle -\\sum_{i=1}^{N} \\log(\\hat{y}_{i,y_i}) \\) Similar to categorical cross-entropy but used when labels are provided as integers rather than one-hot encoded vectors. Useful for multi-class classification with integer labels. Balanced Cross-Entropy \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ w_1 y_i \\log(\\hat{y}_i) + w_0 (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\) Adjusts the standard cross-entropy loss to account for class imbalance by weighting classes inversely proportional to their frequency. Useful in imbalanced classification tasks. Kullback-Leibler Divergence \\( \\displaystyle D_{KL}(P \\| Q) = \\sum_{i} P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right) \\) Measures how one probability distribution diverges from a second, expected probability distribution. It is often used in variational autoencoders and other probabilistic models. Useful in scenarios involving probabilistic models and distributions. Hinge Loss \\( \\displaystyle \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y}_i) \\) Used for \"maximum-margin\" classification, primarily for support vector machines (SVMs). It is designed to ensure that the correct class is not only predicted but also separated from the decision boundary by a margin. Effective for SVMs and tasks requiring a margin between classes. Focal Loss \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\alpha_t (1 - p_t)^\\gamma \\log(p_t) \\) A modified version of cross-entropy loss that addresses class imbalance by down-weighting easy examples and focusing training on hard negatives. Beneficial in scenarios with significant class imbalance, such as object detection. Multi-Class Log Loss \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) \\) Extends binary log loss to multi-class classification problems, penalizing incorrect predictions based on predicted probabilities. Suitable for multi-class classification tasks. Hamming Loss \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{L} \\sum_{j=1}^L \\mathbf{1}(y_{ij} \\neq \\hat{y}_{ij}) \\) Measures the fraction of incorrect labels to the total number of labels, useful for multi-label classification tasks. Effective for multi-label classification scenarios."},{"location":"classes/metrics/classification/#additional","title":"Additional","text":""},{"location":"classes/metrics/classification/#explanation-of-roc-curve-auc-roc","title":"Explanation of ROC Curve (AUC-ROC)","text":"<p>An ROC curve plots the True Positive Rate (TPR, or sensitivity/recall) against the False Positive Rate (FPR) at various classification thresholds. It helps visualize the trade-off between sensitivity and specificity for a classifier:</p> <ul> <li> <p>True Positive Rate (TPR): The proportion of actual positives correctly identified (TP / (TP + FN)).</p> </li> <li> <p>False Positive Rate (FPR): The proportion of actual negatives incorrectly classified as positives (FP / (FP + TN)).</p> </li> <li> <p>The Area Under the Curve (AUC) quantifies the overall performance, with AUC = 1 indicating a perfect classifier and AUC = 0.5 indicating a random classifier.</p> </li> </ul> 2025-11-14T11:31:32.358817 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <ol> <li> <p> Confusion Matrix \u21a9</p> </li> </ol>"},{"location":"classes/metrics/exercise/","title":"5.3. Exercise","text":""},{"location":"classes/metrics/exercise/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p> 02.oct 23:59</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize os algoritmos de KNN e K-Means para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio M\u00e9tricas. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 10 2 Aplica\u00e7\u00e3o das T\u00e9cnicas Implementa\u00e7\u00e3o dos algoritmos KNN e K-Means para treinar modelos de classifica\u00e7\u00e3o. 10 3 Matrizes de Confus\u00e3o Gera\u00e7\u00e3o de matrizes de confus\u00e3o para cada modelo. 20 4 Avalia\u00e7\u00e3o dos Modelos Utiliza\u00e7\u00e3o de m\u00e9tricas como acur\u00e1cia, precis\u00e3o, recall e F1-score para avaliar o desempenho dos modelos. 20 5 Compara\u00e7\u00e3o dos Resultados Compara\u00e7\u00e3o e tabula\u00e7\u00e3o dos resultados obtidos pelos dois algoritmos, discutindo suas vantagens e desvantagens. 20 6 Documenta\u00e7\u00e3o Relat\u00f3rio detalhado do processo, incluindo c\u00f3digo comentado, visualiza\u00e7\u00f5es e conclus\u00f5es. 20"},{"location":"classes/metrics/regression/","title":"5.2. Regression","text":"<p>Regression tasks predict continuous values. The following metrics evaluate the accuracy of predicted values against true values:</p> Metric Purpose Use Case Mean Absolute Error (MAE) \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N \\vert y_i - \\hat{y}_i \\vert \\) Measures average absolute difference between predictions and true values Robust to outliers, interpretable as average error Mean Squared Error (MSE) \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\) Measures average squared difference between predictions and true values Sensitive to outliers, commonly used in neural network loss functions Root Mean Squared Error (RMSE) \\( \\displaystyle \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2} \\) Square root of MSE, providing error in same units as target Preferred for interpretable error magnitude, widely used in forecasting Mean Absolute Percentage Error (MAPE) \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N \\left \\vert \\frac{y_i - \\hat{y}_i}{y_i} \\right \\vert \\cdot 100 \\) Measures average percentage error relative to true values Useful when relative errors matter (e.g., financial predictions), but sensitive to zero or near-zero true values \\(R^2\\) (Coefficient of Determination) \\( \\displaystyle 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2} \\) Measures proportion of variance in dependent variable explained by model Indicates model fit, with values closer to 1 indicating better fit Adjusted \\(R^2\\) \\( \\displaystyle 1 - \\left( \\frac{(1 - R^2)(N - 1)}{N - k - 1} \\right) \\) Adjusts R\u00b2 for number of predictors, penalizing overly complex models Useful when comparing models with different numbers of features Median Absolute Error (\\(\\text{MedAE}\\)) \\( \\displaystyle \\text{median}(\\vert y_1 - \\hat{y}_1 \\vert, \\dots, \\vert y_N - \\hat{y}_N \\vert) \\) Measures median of absolute differences, highly robust to outliers Preferred in datasets with extreme values or non-Gaussian errors"},{"location":"classes/naive_bayes/","title":"9. Na\u00efve Bayes","text":"<p>Naive Bayes Classifier is a family of probabilistic machine learning algorithms used primarily for classification tasks. It's based on Bayes' Theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event.</p> <p>The \"naive\" part comes from the strong assumption that the features (predictors) in the dataset are independent of each other given the class label. This simplifies calculations significantly, making the algorithm efficient even for large datasets.</p> <p>In essence, Naive Bayes calculates the probability that a given instance belongs to a particular class and assigns it to the class with the highest posterior probability. It's particularly popular in text classification because it handles high-dimensional data well (e.g., word counts in documents).</p> <p>Key variants include:</p> <ul> <li>Gaussian Naive Bayes: Assumes features follow a normal distribution (for continuous data).</li> <li>Multinomial Naive Bayes: Suited for discrete data, like word frequencies in text.</li> <li>Bernoulli Naive Bayes: For binary/boolean features, like word presence/absence.</li> </ul> <p>Naive Bayes is derived from Bayes' Theorem:</p> \\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\] <p>In classification terms:</p> <ul> <li>Let \\( C \\) be the class label.</li> <li>Let \\( X = (x_1, x_2, \\dots, x_n) \\) be the feature vector.</li> </ul> <p>We want the posterior probability \\( P(C|X) \\), and we classify \\( X \\) to the class \\( C_k \\) that maximizes this:</p> \\[ P(C_k|X) = \\frac{P(X|C_k) \\cdot P(C_k)}{P(X)} \\] <p>Since \\( P(X) \\) is constant for all classes, we can ignore it and focus on maximizing \\( P(X|C_k) \\cdot P(C_k) \\).</p> <p>The naive assumption: Features are conditionally independent given the class, so:</p> \\[ P(X|C_k) = P(x_1|C_k) \\cdot P(x_2|C_k) \\cdot \\dots \\cdot P(x_n|C_k) = \\prod_{i=1}^n P(x_i|C_k) \\] <ul> <li>Prior \\( P(C_k) \\): Probability of class \\( C_k \\) in the training data (e.g., fraction of samples in that class).</li> <li>Likelihood \\( P(x_i|C_k) \\): Depends on the variant:<ul> <li>For Multinomial: \\( P(x_i|C_k) = \\frac{N_{ki} + \\alpha}{N_k + \\alpha \\cdot V} \\) (with Laplace smoothing, where \\( \\alpha \\) is the smoothing parameter, \\( N_{ki} \\) is count of feature i in class k, \\( N_k \\) is total counts in class k, V is vocabulary size).</li> <li>For Gaussian: \\( P(x_i|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{ki}^2}} \\exp\\left( -\\frac{(x_i - \\mu_{ki})^2}{2\\sigma_{ki}^2} \\right) \\) (mean \\( \\mu \\) and variance \\( \\sigma^2 \\) estimated from training data).</li> </ul> </li> </ul> <p>To avoid zero probabilities (when a feature doesn't appear in a class), we use additive smoothing (e.g., Laplace with \\( \\alpha = 1 \\)).</p> <p>The final prediction is:</p> \\[ \\hat{C} = \\arg\\max_{C_k} P(C_k) \\prod_{i=1}^n P(x_i|C_k) \\] <p>(Often computed in log space to avoid underflow: \\( \\log P(C_k) + \\sum_{i=1}^n \\log P(x_i|C_k) \\).)</p>"},{"location":"classes/naive_bayes/#implementation","title":"Implementation","text":"ResultCode <p>Accuracy: 0.9814814814814815 </p> <pre><code>from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.datasets import fetch_20newsgroups  # Example dataset\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load sample text data\nnewsgroups = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'soc.religion.christian'])\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(newsgroups.data)\ny = newsgroups.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n</code></pre>"},{"location":"classes/naive_bayes/#applications","title":"Applications","text":"<p>Naive Bayes is widely used due to its simplicity and speed:</p> <ul> <li>Spam Detection: Classify emails as spam/ham based on word frequencies (e.g., Gmail's early filters).</li> <li>Sentiment Analysis: Determine if reviews are positive/negative by analyzing text features.</li> <li>Document Classification: Categorize news articles into topics like sports, politics.</li> <li>Medical Diagnosis: Predict diseases from symptoms (assuming independence).</li> <li>Recommendation Systems: Basic user preference prediction.</li> <li>Real-Time Prediction: Fraud detection in finance, where quick decisions are needed on high-dimensional data.</li> </ul> <p>It's especially effective in natural language processing (NLP) with bag-of-words models.</p>"},{"location":"classes/naive_bayes/#pros-and-cons","title":"Pros and Cons","text":""},{"location":"classes/naive_bayes/#pros","title":"Pros","text":"<ul> <li> <p>Simple and Fast</p> <p>Easy to implement, trains quickly (O(n) time), and predicts in constant time relative to data size.</p> </li> <li> <p>Handles High Dimensions</p> <p>Performs well with many features, like in text data (curse of dimensionality resistant).</p> </li> <li> <p>Scalable</p> <p>Can be applied to large datasets and updated easily with new data.</p> </li> <li> <p>Interpretable</p> <p>Probabilistic nature allows understanding of feature contributions to predictions.</p> </li> <li> <p>Good with Small Datasets</p> <p>Requires less training data than complex models.</p> </li> <li> <p>Probabilistic Output</p> <p>Provides probability estimates, not just labels.</p> </li> <li> <p>Robust to Irrelevant Features</p> <p>The independence assumption helps ignore noise.</p> </li> </ul>"},{"location":"classes/naive_bayes/#cons","title":"Cons","text":"<ul> <li> <p>Independence Assumption</p> <p>Rarely holds in real data (e.g., words like \"machine\" and \"learning\" are correlated), leading to suboptimal performance.</p> </li> <li> <p>Zero Probability Problem</p> <p>If a feature-class combo is missing in training, probability becomes zero (mitigated by smoothing, but not perfect).</p> </li> <li> <p>Poor for Continuous Data Without Proper Variant</p> <p>Multinomial works for counts, but Gaussian assumes normality, which may not fit.</p> </li> <li> <p>Biased Estimates</p> <p>Posterior probabilities are often inaccurate, though classification can still be good.</p> </li> <li> <p>Sensitive to Data Representation</p> <p>In text, needs careful preprocessing (e.g., stemming, stop words).</p> </li> </ul>"},{"location":"classes/naive_bayes/#additional","title":"Additional","text":""},{"location":"classes/naive_bayes/#play-golf","title":"Play Golf","text":"<p>A classic example used to illustrate Naive Bayes is predicting whether to play golf based on weather conditions. The dataset includes features like Outlook (Sunny, Overcast, Rain), Temperature (Hot, Mild, Cool), Humidity (High, Normal), and Windy (True, False), along with the target variable Play (Yes, No).</p> Outlook Temperature Humidity Windy Play 1 Sunny Hot High False No 2 Sunny Hot High True No 3 Overcast Hot High False Yes 4 Rain Mild High False Yes 5 Rain Cool Normal False Yes 6 Rain Cool Normal True No 7 Overcast Cool Normal True Yes 8 Sunny Mild High False No 9 Sunny Cool Normal False Yes 10 Rain Mild Normal False Yes 11 Sunny Mild Normal True Yes 12 Overcast Mild High True Yes 13 Overcast Hot Normal False Yes 14 Rain Mild High True No <p>Using this dataset, we can calculate the probabilities needed to predict whether to play golf given specific weather conditions using the Naive Bayes approach.</p>"},{"location":"classes/naive_bayes/#the-goal","title":"The goal","text":"<p>The objective is to predict whether a person will play golf on a new, unseen day based on its weather conditions, such as:</p> <p>Outlook = Sunny,</p> <p>Temperature = Cool,</p> <p>Humidity = High,</p> <p>and Windy = True.</p> <p>To make this prediction, we will calculate the posterior probabilities for both classes (Play = Yes and Play = No) using the Naive Bayes formula and then compare them.</p>"},{"location":"classes/naive_bayes/#calculations-steps","title":"Calculations steps","text":"<ol> <li> <p>Calculate prior probabilities:</p> <ul> <li> <p>P(Play = Yes): 9 out of 14 days were \"Yes.\"</p> <p>\\( \\displaystyle P(\\text{Play} = \\text{Yes}) = \\frac{\\text{Number of Yes}}{\\text{Total}} = \\frac{9}{14} = 0.643 \\)</p> </li> <li> <p>P(Play = No): 5 out of 14 days were \"No.\"</p> <p>\\( \\displaystyle P(\\text{Play} = \\text{No}) = \\frac{\\text{Number of No}}{\\text{Total}} = \\frac{5}{14} = 0.357 \\)</p> </li> </ul> </li> <li> <p>Calculate likelihood probabilities:</p> <p>For each feature given the class, we calculate the likelihoods:</p> Feature P(Feature=Value|Play=Yes) P(Feature=Value|Play=No) Outlook=Sunny \\( \\displaystyle P(\\text{Sunny}\\|\\text{Yes}) = \\frac{2}{9} \\) \\( \\displaystyle P(\\text{Sunny}\\|\\text{No}) = \\frac{3}{5} \\) Temperature=Cool \\( \\displaystyle P(\\text{Cool}\\|\\text{Yes}) = \\frac{3}{9} \\) \\( \\displaystyle P(\\text{Cool}\\|\\text{No}) = \\frac{1}{5} \\) Humidity=High \\( \\displaystyle P(\\text{High}\\|\\text{Yes}) = \\frac{3}{9} \\) \\( \\displaystyle P(\\text{High}\\|\\text{No}) = \\frac{4}{5} \\) Windy=True \\( \\displaystyle P(\\text{True}\\|\\text{Yes}) = \\frac{3}{9} \\) \\( \\displaystyle P(\\text{True}\\|\\text{No}) = \\frac{3}{5} \\) </li> <li> <p>Calculate posterior probabilities (Bayes' theorem):</p> <p>Using the naive assumption that all features are independent, apply the theorem to find the posterior probability for each class. The simplified formula used is:</p> \\[ P(C|X) \\propto P(C) \\cdot \\prod_{i} P(x_i|C) \\] <ul> <li> <p>P(Yes | Sunny, Cool, High, True):</p> <p>\\(\\begin{align*} = &amp; P(\\text{Yes}) \\cdot P(\\text{Sunny}|\\text{Yes}) \\cdot P(\\text{Cool}|\\text{Yes}) \\cdot P(\\text{High}|\\text{Yes}) \\cdot P(\\text{True}|\\text{Yes}) \\\\ = &amp; \\frac{9}{14} \\cdot \\frac{2}{9} \\cdot \\frac{3}{9} \\cdot \\frac{3}{9} \\cdot \\frac{3}{9} \\\\ \\approx &amp; 0.0053 \\end{align*}\\)</p> </li> <li> <p>P(No | Sunny, Cool, High, True):</p> <p>\\(\\begin{align*} = &amp; P(\\text{No}) \\cdot P(\\text{Sunny}|\\text{No}) \\cdot P(\\text{Cool}|\\text{No}) \\cdot P(\\text{High}|\\text{No}) \\cdot P(\\text{True}|\\text{No}) \\\\ = &amp; \\frac{5}{14} \\cdot \\frac{3}{5} \\cdot \\frac{1}{5} \\cdot \\frac{4}{5} \\cdot \\frac{3}{5} \\\\ \\approx &amp; 0.02056 \\end{align*}\\)</p> </li> </ul> </li> <li> <p>Make the prediction:</p> <p>Since \\( P(\\text{No} | \\text{Sunny, Cool, High, True}) \\approx 0.02056 \\) is greater than \\( P(\\text{Yes} | \\text{Sunny, Cool, High, True}) \\approx 0.0053 \\), we predict that the person will not play golf on that day.</p> </li> </ol>"},{"location":"classes/page-rank/","title":"10. Page Rank","text":"<p>PageRank is an algorithm developed by Larry Page and Sergey Brin in the late 1990s, which became the foundation of Google's search engine ranking system. It models the web as a directed graph, where web pages are nodes and hyperlinks are directed edges. The core idea is to estimate the importance (or \"authority\") of a page based on the quantity and quality of links pointing to it. Intuitively, PageRank simulates a \"random surfer\" who browses the web by following links and occasionally jumping to a random page. Pages that are frequently visited in this model are deemed more important.</p> <p>PageRank revolutionized web search by providing a way to rank results objectively, rather than relying solely on keyword matches.</p>"},{"location":"classes/page-rank/#the-random-surfer-model","title":"The Random Surfer Model","text":"<p>Imagine a user starting on a random web page and repeatedly doing one of two things:</p> <ul> <li> <p>Follow a link: With probability \\(1 - d\\) (where \\(d\\) is the damping factor, typically 0.85), the surfer clicks a random outgoing link from the current page.</p> </li> <li> <p>Teleport: With probability \\(d\\), the surfer gets bored and jumps to any random page in the web (uniformly chosen).</p> </li> </ul> <p>This process continues indefinitely. The PageRank of a page is the long-term probability that the surfer lands on it after many steps. This probability stabilizes into a steady-state distribution, which we can compute mathematically.</p>"},{"location":"classes/page-rank/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>Let\u2019s formalize this. Suppose there are \\(n\\) pages in the web, labeled \\(1\\) to \\(n\\). Let \\(p_i\\) be the PageRank of page \\(i\\).</p> <ul> <li>\\(L_i\\): The number of outgoing links from page \\(i\\).</li> <li>The transition probability from page \\(j\\) to page \\(i\\) is \\(\\displaystyle \\frac{1}{L_j}\\) if \\(j\\) links to \\(i\\), else 0.</li> </ul> <p>The PageRank equation for page \\(i\\) is:</p> \\[ p_i = \\frac{d}{n} + d \\sum_{j \\in \\text{links to } i} \\frac{p_j}{L_j} \\] <ul> <li>The first term \\(\\displaystyle \\frac{d}{n}\\) accounts for the random teleport (equal chance for any page).</li> <li>The second term sums the PageRank \"votes\" from pages linking to \\(i\\), weighted by how many outgoing links those pages have (to avoid spamming via link farms).</li> </ul> <p>This is a system of linear equations: \\(\\displaystyle \\mathbf{p} = d \\mathbf{E} \\mathbf{p} + \\frac{d}{n} \\mathbf{1}\\), where \\(\\mathbf{E}\\) is the transpose of the adjacency matrix (column-stochastic, normalized by out-degrees), and \\(\\mathbf{1}\\) is a vector of ones. Solving for the eigenvector gives the PageRank vector \\(\\mathbf{p}\\) with \\(\\displaystyle \\sum p_i = 1\\).</p> <p>The damping factor \\(d \\approx 0.85\\) ensures convergence (the Markov chain is irreducible and aperiodic) and models real browsing (about 15% random jumps).</p>"},{"location":"classes/page-rank/#the-computation-process","title":"The Computation Process","text":"<p>To compute PageRank:</p> <ol> <li>Initialize: Set all \\(p_i = \\displaystyle \\frac{1}{n}\\).</li> <li>Iterate: Repeatedly apply the PageRank formula until convergence (e.g., changes &lt; \u03b5, like 0.0001).</li> <li>Power Iteration: This is essentially the power method for finding the dominant eigenvector of the transition matrix \\(\\displaystyle M = d \\mathbf{E} + \\frac{(1-d)}{n} \\mathbf{J}\\) (where \\(\\mathbf{J}\\) is all-ones matrix).</li> </ol> <p>For large graphs (billions of pages), Google uses optimized sparse matrix techniques and distributed computing. Convergence typically takes 20\u201350 iterations.</p>"},{"location":"classes/page-rank/#simple-example","title":"Simple Example","text":"<p>Consider a tiny web with 3 pages:</p> <ul> <li>Page A links to B and C.</li> <li>Page B links to C.</li> <li>Page C links to A.</li> </ul> <p>Adjacency: A \u2192 B,C; B \u2192 C; C \u2192 A.</p> <pre><code>graph TD;\n    A --&gt; B\n    A --&gt; C\n    B --&gt; C\n    C --&gt; A</code></pre> <p>Using \\(d=0.85\\), \\(n=3\\):</p> <p>Initial: \\(p_A = p_B = p_C = 1/3 \\approx 0.333\\).</p> <p>After iterations (approximate steady state):</p> <ul> <li>\\(p_A \\approx 0.382\\)</li> <li>\\(p_B \\approx 0.208\\)</li> <li>\\(p_C \\approx 0.410\\)</li> </ul> <p>Page C ranks highest because it's part of a cycle and gets \"votes\" from both A and B.</p> Iteration p_A p_B p_C 0 0.333 0.333 0.333 1 0.368 0.215 0.417 2 0.380 0.208 0.412 3 0.382 0.208 0.410 Converged 0.382 0.208 0.410 <p>(This table shows how values stabilize; in practice, you'd solve the exact system.)</p>"},{"location":"classes/page-rank/#key-properties-and-limitations","title":"Key Properties and Limitations","text":"<ul> <li> <p>Strengths</p> <ul> <li>Robust to spam: Quality links from authoritative pages carry more weight.</li> <li>Global view: Considers the entire web structure.</li> <li>Scalable: Linear algebra makes it efficient.</li> </ul> </li> <li> <p>Limitations (addressed in modern variants like Personalized PageRank)</p> <ul> <li>Ignores content: Purely link-based.</li> <li>Link farms: Groups of pages mutually linking to inflate ranks (mitigated by penalties).</li> <li>Dead ends: Pages with no outgoing links (handled by assuming they teleport).</li> <li>Spam evolution: Modern Google combines PageRank with hundreds of signals (e.g., machine learning).</li> </ul> </li> </ul> <p>PageRank's influence extends beyond search\u2014to recommendation systems, social networks, and graph analytics. It's a cornerstone of spectral graph theory. For deeper dives, the original 1998 paper \"The PageRank Citation Ranking\" is a great read.</p>"},{"location":"classes/page-rank/#additional","title":"Additional","text":""},{"location":"classes/page-rank/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p> 07.nov 23:59</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>O objetivo deste exerc\u00edcio \u00e9 implementar e aplicar o algoritmo PageRank em um grafo dirigido (ou adaptado de um grafo n\u00e3o dirigido, tratando arestas como bidirecionais). O PageRank \u00e9 um algoritmo cl\u00e1ssico para medir a import\u00e2ncia de n\u00f3s em uma rede, baseado na estrutura de links (ou rela\u00e7\u00f5es). Ele pode ser usado para ranquear p\u00e1ginas web, mas aqui focamos em aplica\u00e7\u00f5es variadas.</p> <p>Tarefa Principal:</p> <ol> <li>Escolha um dos datasets sugeridos (ou equivalente) de pelo menos um dos tipos de dados propostos;</li> <li>Carregue o grafo usando uma biblioteca como NetworkX (em Python) ou equivalente;</li> <li>Implemente o algoritmo PageRank do zero (usando a f\u00f3rmula iterativa: \\( \\displaystyle PR(p_i) = \\frac{1-d}{N} + d \\sum_{p_j \\in M(p_i)} \\frac{PR(p_j)}{L(p_j)} \\), onde \\( d \\) \u00e9 o fator de amortecimento, tipicamente 0.85; \\( N \\) \u00e9 o n\u00famero de n\u00f3s; \\( M(p_i) \\) s\u00e3o os n\u00f3s que apontam para \\( p_i \\); \\( L(p_j) \\) \u00e9 o n\u00famero de sa\u00eddas de \\( p_j \\));</li> <li>Compute os valores de PageRank para os n\u00f3s do grafo (use um crit\u00e9rio de converg\u00eancia, como diferen\u00e7a menor que 0.0001 entre itera\u00e7\u00f5es);</li> <li>Compare os resultados com uma implementa\u00e7\u00e3o pronta (ex.: <code>networkx.pagerank</code>);</li> <li>Analise os resultados: identifique os 10 n\u00f3s mais importantes e explique o que eles representam no contexto do dataset (ex.: por que certos n\u00f3s s\u00e3o mais \"influentes\"?);</li> <li>Varie o fator de amortecimento (ex.: 0.5, 0.85, 0.99) e discuta o impacto nos rankings.</li> </ol>"},{"location":"classes/page-rank/#sugestoes-de-datasets-e-formas-de-aplicacao","title":"Sugest\u00f5es de Datasets e Formas de Aplica\u00e7\u00e3o","text":"<p>Escolha pelo menos um dataset de cada tipo para variedade, mas o exerc\u00edcio pode ser feito com apenas um. Todos os datasets sugeridos s\u00e3o p\u00fablicos e gratuitos, dispon\u00edveis em reposit\u00f3rios como Stanford SNAP (https://snap.stanford.edu/data/) ou Network Repository (https://networkrepository.com/). Baixe os arquivos de arestas (geralmente em formato .txt ou .edges) e carregue como grafo dirigido ou n\u00e3o dirigido no NetworkX.</p> <ol> <li> <p>Tipo de Dados: Rede Social (ex.: rede de confian\u00e7a ou amizades)</p> <ul> <li> <p>Dataset Sugerido: soc-Epinions1 (da Epinions, uma rede de confian\u00e7a entre usu\u00e1rios de um site de reviews). Dispon\u00edvel em SNAP: https://snap.stanford.edu/data/soc-Epinions1.html. Tamanho: ~76k n\u00f3s, ~509k arestas dirigidas (usu\u00e1rios confiam em outros).</p> </li> <li> <p>Forma de Aplica\u00e7\u00e3o: Modele o grafo como dirigido, onde uma aresta A \u2192 B significa que A confia em B. Aplique PageRank para identificar usu\u00e1rios mais \"influentes\" (aqueles com mais confian\u00e7a indireta). Interprete: N\u00f3s com alto PageRank s\u00e3o influenciadores na comunidade, pois recebem confian\u00e7a de fontes confi\u00e1veis. Use para analisar propaga\u00e7\u00e3o de opini\u00f5es ou recomenda\u00e7\u00f5es sociais.</p> </li> </ul> </li> <li> <p>Tipo de Dados: Rede de Cita\u00e7\u00f5es Acad\u00eamicas (ex.: papers citando outros)</p> <ul> <li> <p>Dataset Sugerido: cit-HepTh (cita\u00e7\u00f5es em papers de F\u00edsica de Alta Energia, do arXiv). Dispon\u00edvel em SNAP: https://snap.stanford.edu/data/cit-HepTh.html. Tamanho: ~28k n\u00f3s (papers), ~353k arestas dirigidas (cita\u00e7\u00f5es).</p> </li> <li> <p>Forma de Aplica\u00e7\u00e3o: Grafo dirigido: uma aresta A \u2192 B significa que paper A cita paper B (B \u00e9 \"importante\" para A). PageRank ranqueia papers mais impactantes (aqueles citados por papers influentes). Interprete: Papers com alto score s\u00e3o fundamentais no campo, como trabalhos seminais. \u00datil para an\u00e1lise de impacto cient\u00edfico ou recomenda\u00e7\u00e3o de leitura.</p> </li> </ul> </li> <li> <p>Tipo de Dados: Rede Biol\u00f3gica (ex.: intera\u00e7\u00f5es entre prote\u00ednas)</p> <ul> <li> <p>Dataset Sugerido: bio-Dmela (intera\u00e7\u00f5es prote\u00edna-prote\u00edna em Drosophila melanogaster, da mosca da fruta). Dispon\u00edvel em Network Repository: https://networkrepository.com/bio-Dmela.php. Tamanho: ~7k n\u00f3s (prote\u00ednas), ~26k arestas n\u00e3o dirigidas (intera\u00e7\u00f5es; adapte para bidirecional).</p> </li> <li> <p>Forma de Aplica\u00e7\u00e3o: Converta o grafo n\u00e3o dirigido em dirigido bidirecional (adicionando arestas em ambas dire\u00e7\u00f5es). Aplique PageRank para identificar prote\u00ednas \"centrais\" na rede biol\u00f3gica. Interprete: Prote\u00ednas com alto PageRank s\u00e3o hubs em pathways metab\u00f3licos ou sinaliza\u00e7\u00e3o celular, potencialmente alvos para estudos de doen\u00e7as. \u00datil em bioinform\u00e1tica para priorizar genes/prote\u00ednas.</p> </li> </ul> </li> </ol> <p>Dicas Gerais para Aplica\u00e7\u00e3o:</p> <ul> <li>Para datasets grandes, use amostras (subgrafos) se o computation for lento.</li> <li>Bibliotecas: NetworkX para grafos, NumPy para c\u00e1lculos matriciais (PageRank pode ser implementado via matriz de transi\u00e7\u00e3o).</li> <li>Se precisar de mais op\u00e7\u00f5es: Outros datasets incluem roadNet-CA (redes de estradas, para an\u00e1lise de tr\u00e1fego) ou ego-Facebook (redes sociais pessoais).</li> </ul>"},{"location":"classes/page-rank/#rubricas-de-correcao","title":"Rubricas de Corre\u00e7\u00e3o","text":"<p>A corre\u00e7\u00e3o ser\u00e1 baseada em uma escala de 0 a 10 pontos, dividida em crit\u00e9rios. O total \u00e9 somado e normalizado. Exija evid\u00eancias no c\u00f3digo e relat\u00f3rio.</p> <ol> <li> <p>Implementa\u00e7\u00e3o do Algoritmo (3 pontos)</p> <ul> <li>3: Implementa\u00e7\u00e3o correta do zero, com itera\u00e7\u00f5es, converg\u00eancia e varia\u00e7\u00e3o de d; compara com biblioteca pronta sem erros.</li> <li>2: Implementa\u00e7\u00e3o funcional, mas com pequenos erros ou sem varia\u00e7\u00e3o de d.</li> <li>1: Implementa\u00e7\u00e3o parcial ou s\u00f3 usando biblioteca pronta.</li> <li>0: N\u00e3o implementado ou com erros graves.</li> </ul> </li> <li> <p>Carregamento e Prepara\u00e7\u00e3o do Dataset (2 pontos)</p> <ul> <li>2: Dataset escolhido corretamente carregado; grafo modelado adequadamente (dirigido/bidirecional); pelo menos um tipo de dados usado.</li> <li>1: Carregamento ok, mas sem adapta\u00e7\u00e3o para o tipo de grafo ou escolha inadequada.</li> <li>0: Erros no carregamento ou dataset irrelevante.</li> </ul> </li> <li> <p>An\u00e1lise e Interpreta\u00e7\u00e3o dos Resultados (3 pontos)</p> <ul> <li>3: Identifica top n\u00f3s com explica\u00e7\u00e3o contextual clara; discute impacto de d; inclui visualiza\u00e7\u00f5es relevantes.</li> <li>2: An\u00e1lise b\u00e1sica, mas superficial ou sem discuss\u00e3o de varia\u00e7\u00f5es.</li> <li>1: Apenas lista resultados sem interpreta\u00e7\u00e3o.</li> <li>0: Sem an\u00e1lise.</li> </ul> </li> <li> <p>Qualidade do C\u00f3digo e Relat\u00f3rio (1 ponto)</p> <ul> <li>1: C\u00f3digo limpo, comentado; relat\u00f3rio bem estruturado, sem pl\u00e1gio.</li> <li>0.5: C\u00f3digo funcional mas bagun\u00e7ado; relat\u00f3rio incompleto.</li> <li>0: C\u00f3digo ileg\u00edvel ou relat\u00f3rio ausente.</li> </ul> </li> <li> <p>Originalidade e Profundidade (1 ponto)</p> <ul> <li>1: Usa mais de um dataset ou explora aplica\u00e7\u00f5es criativas; insights al\u00e9m do b\u00e1sico.</li> <li>0.5: Atende ao m\u00ednimo.</li> <li>0: C\u00f3pia ou superficial.</li> </ul> </li> </ol> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio Page Rank. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <ol> <li> <p>Wikipedia: PageRank \u21a9</p> </li> <li> <p>GeeksforGeeks: PageRank Algorithm and Implementation \u21a9</p> </li> </ol>"},{"location":"classes/preprocessing/","title":"2. Preprocessing","text":"<p>Data preprocessing is a critical phase in the development of neural network models, ensuring that raw data is transformed into a suitable format for effective training and inference. This text explores both basic and advanced preprocessing techniques, drawing from established methodologies in machine learning and deep learning. Basic techniques focus on cleaning and normalizing data to handle inconsistencies and scale issues, while advanced methods address complex challenges such as data scarcity, imbalance, and high dimensionality. The discussion highlights their relevance to neural networks, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers, with emphasis on improving model convergence, generalization, and performance.</p> <p>Neural networks, as powerful function approximators, are highly sensitive to the quality and format of input data. Poorly prepared data can lead to slow convergence, overfitting, or suboptimal accuracy. Preprocessing mitigates these issues by addressing noise, inconsistencies, and structural mismatches in datasets. It encompasses a series of steps that transform raw data into a form that aligns with the assumptions and requirements of neural architectures. For instance, in supervised learning tasks, preprocessing ensures features are scaled appropriately to prevent gradient issues during backpropagation. This text delineates basic techniques, which are foundational and widely applicable, and advanced techniques, which are more specialized and often domain-specific, such as for image, text, or time-series data.</p>"},{"location":"classes/preprocessing/#typical-preprocessing-tasks","title":"Typical Preprocessing Tasks","text":"Task Description Text Cleaning Remove unwanted characters, stop words, and perform stemming/lemmatization. Normalization Standardize text formats, such as date and currency formats. Tokenization Split text into words or subwords for easier analysis. Feature Extraction Convert text into numerical features using techniques like TF-IDF or word embeddings. Data Augmentation Generate synthetic data to increase dataset size and diversity. <p>A typical dataset for machine learning tasks might include columns of different data types, such as numerical, categorical, and text, eg.:</p> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 91 0 3 Christmann, Mr. Emil male 29 0 0 343276 8.05 nan S 665 1 3 Lindqvist, Mr. Eino William male 20 1 0 STON/O 2. 3101285 7.925 nan S 879 0 3 Laleff, Mr. Kristo male nan 0 0 349217 7.8958 nan S 331 1 3 McCoy, Miss. Agnes female nan 2 0 367226 23.25 nan Q 445 1 3 Johannesen-Bratthammer, Mr. Bernt male nan 0 0 65306 8.1125 nan S 166 1 3 Goldsmith, Master. Frank John William \"Frankie\" male 9 0 2 363291 20.525 nan S 464 0 2 Milling, Mr. Jacob Christian male 48 0 0 234360 13 nan S 194 1 2 Navratil, Master. Michel M male 3 1 1 230080 26 F2 S 882 0 3 Markun, Mr. Johann male 33 0 0 349257 7.8958 nan S 625 0 3 Bowen, Mr. David John \"Dai\" male 21 0 0 54636 16.1 nan S <p>Sample rows from the Titanic dataset</p>"},{"location":"classes/preprocessing/#data-cleaning","title":"Data Cleaning","text":"<p>Data cleaning involves identifying and rectifying errors, inconsistencies, and missing values in the dataset. Missing values, common in real-world data, can be handled by imputation methods such as mean, median, or mode substitution, or by removing affected rows/columns if the loss is minimal. For example, in pandas, this can be implemented as <code>df.fillna(df.mean())</code> for mean imputation. Outliers, which may skew neural network training, are detected using statistical methods like z-scores or interquartile ranges and can be winsorized or removed. Noise reduction, such as smoothing time-series data with moving averages, is also essential, particularly for RNNs where temporal dependencies are critical. Inconsistent data, like varying formats in text (e.g., dates), requires standardization to ensure uniformity. Overall, data cleaning enhances data quality, reducing the risk of misleading patterns during neural network optimization.</p> ResultCode Pclass Sex Age SibSp Parch Fare Embarked 3 male 38.5 0 0 8.05 S 2 male 42 0 0 13 S 3 female 47 1 0 14.5 S 2 female 38 0 0 13 S 3 female 30 0 0 12.475 S 1 male 31 1 0 52 S 3 female 18 2 0 18 S 1 female 40 0 0 153.463 S 2 male 38.5 0 0 13.8625 C 3 female 39 1 5 31.275 S <pre><code>import pandas as pd\n\n# Preprocess the data\ndef preprocess(df):\n    # Fill missing values\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n\n    # Select features\n    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n    return df[features]\n\n# Load the Titanic dataset\ndf = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv')\ndf = df.sample(n=10)\n\n# Preprocessing\ndf = preprocess(df)\n\n# Display the first few rows of the dataset\nprint(df.to_markdown(index=False))\n</code></pre>"},{"location":"classes/preprocessing/#encoding-categorical-variables","title":"Encoding Categorical Variables","text":"<p>Categorical data, non-numeric by nature, must be converted for neural network input. One-hot encoding creates binary vectors for each category, e.g., transforming colors <code>['red', 'blue', 'green']</code> into <code>[[1,0,0], [0,1,0], [0,0,1]]</code>. This avoids ordinal assumptions but increases dimensionality, which can be mitigated by embedding layers in neural networks for high-cardinality features. Label encoding assigns integers (e.g., 0 for \"red\", 1 for \"blue\"), suitable for ordinal categories but risky for nominal ones due to implied ordering. For text data in NLP tasks with transformers, tokenization and subword encoding (e.g., WordPiece) are basic steps to map words to integer IDs.</p> ResultCode Pclass Sex Age SibSp Parch Fare Embarked 3 0 21 2 2 34.375 2 2 0 30 0 0 12.35 1 2 0 25 0 1 26 2 1 0 2 1 2 151.55 2 1 1 65 0 0 26.55 2 3 0 18 0 0 7.4958 2 2 1 3 1 1 18.75 2 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Preprocess the data\ndef preprocess(df):\n    # Fill missing values\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n\n    # Convert categorical variables\n    label_encoder = LabelEncoder()\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])\n\n    # Select features\n    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n    return df[features]\n\n# Load the Titanic dataset\ndf = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv')\ndf = df.sample(n=10)\n\n# Preprocessing\ndf = preprocess(df)\n\n# Display the first few rows of the dataset\nprint(df.sample(n=7).to_markdown(index=False))\n</code></pre>"},{"location":"classes/preprocessing/#normalization-and-standardization","title":"Normalization and Standardization","text":"<p>Normalization scales features to a bounded range, typically \\([0, 1]\\), using min-max scaling:</p> \\[ x' = \\displaystyle \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\] <p>This is crucial for neural networks employing sigmoid or tanh activations, as it prevents saturation.</p> <p>Standardization, or z-score normalization, transforms data to have a mean of \\(0\\) and standard deviation of \\(1\\):</p> \\[ x' = \\frac{x - \\mu}{\\sigma}, \\] <p>where \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation. It is preferred for networks with ReLU activations or when data distributions are Gaussian-like, aiding faster gradient descent convergence. In practice, libraries like scikit-learn provide <code>MinMaxScaler</code> and <code>StandardScaler</code> for these operations. These techniques are especially vital in multilayer perceptrons (MLPs) and CNNs, where feature scales can dominate loss landscapes.</p> <p>Below is an example of how to apply normalization and standardization using pandas, based on the NASDAQ Apple stock price dataset:</p> ResultOriginalCode Date Volume N-Volume Z-Volume Change N-Change Z-Change 2025-10-15 00:00:00-04:00 3.38936e+07 0.0197252 -1.0616 0.00633653 0.407608 0.146478 2025-10-16 00:00:00-04:00 3.9777e+07 0.121641 -0.655312 -0.00758001 0.15856 -0.979042 2025-10-17 00:00:00-04:00 4.9147e+07 0.283954 -0.00825403 0.0195594 0.644243 1.2159 2025-10-20 00:00:00-04:00 9.0483e+07 1 2.84626 0.0394388 1 2.82367 2025-10-21 00:00:00-04:00 4.66959e+07 0.241494 -0.177518 0.00202102 0.330378 -0.202545 2025-10-22 00:00:00-04:00 4.50153e+07 0.212382 -0.293574 -0.0164402 0 -1.69562 2025-10-23 00:00:00-04:00 3.27549e+07 0 -1.14023 0.0043721 0.372453 -0.0123976 2025-10-24 00:00:00-04:00 3.82537e+07 0.0952534 -0.760505 0.0124819 0.517584 0.643492 2025-10-27 00:00:00-04:00 4.48882e+07 0.21018 -0.302351 0.0227911 0.702076 1.47727 2025-10-28 00:00:00-04:00 4.15348e+07 0.152091 -0.533925 0.000706831 0.30686 -0.308832 Date Open High Low Close Volume Dividends Stock Splits Change 2025-10-14 00:00:00-04:00 246.361 248.609 244.463 247.53 3.5478e+07 0 0 nan 2025-10-15 00:00:00-04:00 249.248 251.576 247.23 249.099 3.38936e+07 0 0 0.00633653 2025-10-16 00:00:00-04:00 248.01 248.799 244.893 247.21 3.9777e+07 0 0 -0.00758001 2025-10-17 00:00:00-04:00 247.78 253.135 247.031 252.046 4.9147e+07 0 0 0.0195594 2025-10-20 00:00:00-04:00 255.642 264.124 255.382 261.986 9.0483e+07 0 0 0.0394388 2025-10-21 00:00:00-04:00 261.626 265.033 261.576 262.516 4.66959e+07 0 0 0.00202102 2025-10-22 00:00:00-04:00 262.396 262.595 255.183 258.2 4.50153e+07 0 0 -0.0164402 2025-10-23 00:00:00-04:00 259.688 260.368 257.76 259.329 3.27549e+07 0 0 0.0043721 2025-10-24 00:00:00-04:00 260.937 263.874 258.929 262.565 3.82537e+07 0 0 0.0124819 2025-10-27 00:00:00-04:00 264.623 268.859 264.394 268.55 4.48882e+07 0 0 0.0227911 <pre><code>import pandas as pd\nimport yfinance as yf\n\ndat = yf.Ticker(\"AAPL\")\ndf = dat.history(period='1mo')\n\ndf['Change'] = df['Close'].pct_change()\ndf['Z-Volume'] = df['Volume'].apply(lambda x: (x-df['Volume'].mean())/df['Volume'].std())\ndf['N-Volume'] = df['Volume'].apply(lambda x: (x-df['Volume'].min())/(df['Volume'].max()-df['Volume'].min()))\ndf['Z-Change'] = df['Change'].apply(lambda x: (x-df['Change'].mean())/df['Change'].std())\ndf['N-Change'] = df['Change'].apply(lambda x: (x-df['Change'].min())/(df['Change'].max()-df['Change'].min()))\ndf = df[['Volume', 'N-Volume', 'Z-Volume', 'Change', 'N-Change', 'Z-Change']].dropna()\nprint(df.head(10).to_markdown())\n</code></pre>"},{"location":"classes/preprocessing/#feature-scaling","title":"Feature Scaling","text":"<p>Feature scaling overlaps with normalization but specifically addresses disparate scales across features. Beyond min-max and z-score, logarithmic scaling (\\( x' = \\log(x + 1) \\)) handles skewed distributions, common in financial data for neural forecasting models. Scaling ensures equal contribution of features during weight updates in stochastic gradient descent (SGD).</p>"},{"location":"classes/preprocessing/#data-augmentation","title":"Data Augmentation","text":"<p>Data augmentation artificially expands datasets to combat overfitting, particularly in CNNs for image classification. Basic operations include flipping, rotation (e.g., by 90\u00b0 or random angles), and cropping, while advanced methods involve adding noise (Gaussian or salt-and-pepper) or color jittering. For text data in RNNs or transformers, techniques like synonym replacement, random insertion/deletion, or back-translation (translating to another language and back) generate variations while preserving semantics. In time-series for LSTMs, window slicing or synthetic minority over-sampling technique (SMOTE)<sup>8</sup> variants create augmented sequences. Generative models like GANs (Generative Adversarial Networks) represent cutting-edge augmentation, producing realistic synthetic samples. These methods improve generalization by exposing models to diverse inputs.</p>"},{"location":"classes/preprocessing/#handling-imbalanced-data","title":"Handling Imbalanced Data","text":"<p>Imbalanced datasets, where classes are unevenly represented, bias neural networks toward majority classes. Advanced resampling includes oversampling minorities (e.g., SMOTE, which interpolates new instances) or undersampling majorities. Class weighting assigns higher penalties to minority misclassifications in the loss function, e.g., weighted cross-entropy. Ensemble methods, like balanced random forests integrated with neural embeddings, or focal loss in object detection CNNs, further address this. For sequential data, temporal resampling ensures balanced windows.</p>"},{"location":"classes/preprocessing/#feature-engineering-and-selection","title":"Feature Engineering and Selection","text":"<p>Feature engineering crafts new features from existing ones, such as polynomial terms or interactions (e.g., \\( x_1 \\times x_2 \\)) to capture non-linearities before neural input. Selection techniques like mutual information or recursive feature elimination reduce irrelevant features, alleviating the curse of dimensionality in high-dimensional data for autoencoders or dense networks. Embedded methods, like L1 regularization in neural training, perform selection during optimization.</p>"},{"location":"classes/preprocessing/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Techniques like Principal Component Analysis (PCA) project data onto lower-dimensional spaces while preserving variance:</p> \\[ X' = X \\cdot W \\] <p>where \\(W\\) are principal components. Autoencoders, a neural-based approach, learn compressed representations through encoder-decoder architectures. t-SNE or UMAP are used for visualization but less for preprocessing due to non-linearity. These are vital for CNNs on high-resolution images or transformers on long sequences to reduce computational load.</p> <p>PCA is widely used for dimensionality reduction<sup>5</sup>, while t-SNE<sup>6</sup> and UMAP<sup>7</sup> are popular for visualizing high-dimensional data in 2D or 3D spaces.</p> <p>Basically, PCA identifies orthogonal axes (principal components) capturing maximum variance, enabling efficient data representation. Autoencoders, trained to reconstruct inputs, learn compact latent spaces, useful for denoising or anomaly detection.</p> <p>PCA Steps<sup>5</sup></p> <p>1. Standardize the data:</p> \\[ X' = \\frac{X - \u03bc}{\u03c3} \\] <p>2. Compute the covariance matrix:</p> \\[ C = \\frac{1}{n} * (X'\u1d40 * X') \\] <p>3. Calculate eigenvalues and eigenvectors:</p> \\[ \\text{eigvals}, \\text{eigvecs} = \\text{np.linalg.eig}(C) \\] <p>4. Sort eigenvectors by eigenvalues in descending order.</p> <p>5. Select top \\(k\\) eigenvectors to form a new feature space</p> \\[ Y = X' * W \\] <p>where \\(W\\) is the matrix of selected eigenvectors.</p> <p>A example of PCA applied to the Iris dataset:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom io import StringIO\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\n\n# Loading Iris dataset\niris = load_iris()\n\n# Transform in dataframe\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\n)\ndf['class'] = iris.target_names[iris.target]\n\nX = df.iloc[:,0:4].values\ny = df.iloc[:,4].values\n\n# Standardizing\nX_std = StandardScaler().fit_transform(X)\n\n# Covariance\ncov_mat = np.cov(X_std.T)\n\n# Calculate autovalues and autovectors\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)\n\n# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs: print(i[0])\n\n# Sum the cummulative of each eigen value\ntot = sum(eig_vals)\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\nn_eigen = [1, 2, 3, 4]\n\n# Plot the cumulative for each eign value\nplt.figure(figsize=(6, 4))\nplt.bar(n_eigen, var_exp, alpha=0.5, align='center',\n    label='individual explained variance')\nplt.step(n_eigen, cum_var_exp, where='mid',\n    label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()\n\n# Take the only the two firsts eigen values\nmatrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n                      eig_pairs[1][1].reshape(4,1)))\n\nprint('*' * 10)\nprint('Reduced to 2-D')\nprint('Matrix W:\\n', matrix_w)\n\n# Calculate the new Y for all samples\nY = X_std.dot(matrix_w)\n\n# Plot the data for the 2 firsts principal components\nplt.figure(figsize=(6, 4))\nfor lab, col in zip(('setosa', 'versicolor', 'virginica'), ('blue', 'red', 'green')):\n    plt.scatter(Y[y==lab, 0],\n                Y[y==lab, 1],\n                label=lab,\n                c=col)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(loc='lower center')\nplt.tight_layout()\n\n# Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n\nplt.close()\n</code></pre> <p>Now, the same example using scikit-learn is shown below:</p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA as pca\nfrom sklearn.preprocessing import StandardScaler\n\n# Loading Iris dataset\niris = load_iris()\n\n# Transform in dataframe\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\n)\ndf['class'] = iris.target_names[iris.target]\n\nX = df.iloc[:,0:4].values\ny = df.iloc[:,4].values\n\n# Standardizing\nX_std = StandardScaler().fit_transform(X)\n\nsklearn_pca = pca(n_components=2)\nY = sklearn_pca.fit_transform(X_std)\n\n# Plot the data for the 2 firsts principal components\nplt.figure(figsize=(6, 4))\nfor lab, col in zip(('setosa', 'versicolor', 'virginica'), ('blue', 'red', 'green')):\n    plt.scatter(Y[y==lab, 0],\n                Y[y==lab, 1],\n                label=lab,\n                c=col)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(loc='lower center')\nplt.tight_layout()\n\n# Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>Eigenfaces, a PCA variant, is used in face recognition tasks to reduce image dimensions while retaining essential features<sup>4</sup>. In NLP, techniques like Latent Semantic Analysis (LSA) apply SVD (Singular Value Decomposition) to reduce term-document matrices, enhancing transformer efficiency.</p>"},{"location":"classes/preprocessing/#domain-specific-advanced-techniques","title":"Domain-Specific Advanced Techniques","text":"<p>For time-series in RNNs, techniques include Fast Fourier Transform (FFT) for frequency domain conversion or segmentation into fixed windows. In text preprocessing for sentiment analysis, advanced steps encompass negation handling (e.g., marking \"not good\" as \"not_pos\"), intensification (e.g., \"very good\" as \"strong_pos\"), and POS tagging to retain sentiment-bearing words. For images in CNNs, advanced signal processing like wavelet transforms or conversion to spectrograms enhances fault diagnosis applications.</p>"},{"location":"classes/preprocessing/#appendix","title":"Appendix","text":""},{"location":"classes/preprocessing/#normalization-vs-standardization-in-neural-network-preprocessing","title":"Normalization vs. Standardization in Neural Network Preprocessing","text":"<p>In data preprocessing for neural networks, both normalization and standardization are feature scaling techniques used to handle features with different scales, improve model convergence, stabilize gradients during training, and prevent features with larger ranges from dominating the learning process. These methods are particularly important for optimization algorithms like gradient descent, which neural networks rely on.</p> <ul> <li> <p>Normalization (Min-Max Scaling): This scales the data to a fixed range, typically [0, 1] or [-1, 1], using the formula: \\( \\displaystyle x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\). It preserves the original data distribution but is sensitive to outliers, as extreme values can compress the rest of the data into a narrow interval.</p> </li> <li> <p>Standardization (Z-Score Scaling): This transforms the data to have a mean of 0 and a standard deviation of 1, using the formula: \\( \\displaystyle z = \\frac{x - \\mu}{\\sigma} \\), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. It centers the data and is more robust to outliers, but it does not bound the values to a specific range.</p> </li> </ul> <p>There is no universal \"better\" method; the choice depends on the data characteristics, the neural network architecture, and empirical testing. If the dataset is small, it's often worth experimenting with both to see which yields better performance. Below, I'll outline guidelines for when each is appropriate, with specific situations or cases, especially in the context of neural networks.</p> <p>When to Use Normalization</p> <p>Normalization is preferred when the data needs to be bounded within a specific range, the distribution is unknown or non-Gaussian, and there are no significant outliers. It helps avoid numeric overflow in neural networks, speeds up learning, and works well with activation functions that expect inputs in a constrained range. Key situations include:</p> <ul> <li> <p>Bounded Data or Activation Functions Sensitive to Range: Use normalization for neural networks with sigmoid or tanh activations, as these functions perform better with inputs scaled to [0, 1] or [-1, 1] to prevent saturation (where gradients become near zero). For example, in image classification tasks with convolutional neural networks (CNNs), pixel values (typically 0-255) are often normalized to [0, 1] to ensure consistent scaling and faster convergence.</p> </li> <li> <p>Features with Known Min/Max Bounds and No Outliers: When the data has clear minimum and maximum values (e.g., sensor readings bounded between fixed limits), normalization prevents larger-scale features from dominating. A case is processing demographic data like age (e.g., 0-100) in a feedforward neural network for prediction tasks, where scaling to [0, 1] maintains proportionality without assuming a normal distribution.</p> </li> <li> <p>General Speed Improvements in Training: In scenarios where neural networks handle features like age and weight, normalization to [0, 1] can accelerate training and testing by keeping inputs small and consistent, reducing the risk of overflow.</p> </li> </ul> <p>When to Use Standardization</p> <p>Standardization is suitable when the data approximates a Gaussian (normal) distribution, outliers are present, or the model benefits from centered data with unit variance. It helps prevent gradient saturation in neural networks, improves numerical stability, and is often the default choice for many algorithms. Specific cases include:</p> <ul> <li> <p>Data with Outliers or Unknown Distribution: Standardization is more robust to outliers, as it doesn't compress values into a fixed range like normalization does. For instance, in financial datasets for stock price prediction using recurrent neural networks (RNNs), where extreme values (e.g., market crashes) are common, standardization preserves the relative importance of outliers without skewing the scale.</p> </li> <li> <p>Gaussian-Like Data or Convergence-Focused Models: When features follow a bell-curve distribution (verifiable by plotting), standardization aligns with assumptions in techniques like batch normalization in deep neural networks. An example is sensor data analysis in IoT applications with neural networks, where standardization ensures faster gradient descent convergence by centering the data.</p> </li> <li> <p>Standard Practice for Neural Networks: As recommended in foundational work like Yann LeCun's efficient backpropagation paper, scaling to mean 0 and variance 1 is a go-to method to avoid saturating hidden units and handle numerical issues in training. This is common in large-scale datasets for tasks like natural language processing with transformers.</p> </li> </ul> <p>In practice, for neural networks, standardization is often preferred as a starting point due to its robustness, but normalization shines in bounded, outlier-free scenarios. Always apply scaling after splitting data into train/test sets to avoid data leakage, and use libraries like scikit-learn's <code>MinMaxScaler</code> or <code>StandardScaler</code> for implementation.</p> <ol> <li> <p>Scikit-learn - Preprocessing data \u21a9</p> </li> <li> <p>TensorFlow - Data Augmentation \u21a9</p> </li> <li> <p>AutoML - Automated Machine Learning \u21a9</p> </li> <li> <p>Face Recognition with OpenCV \u21a9</p> </li> <li> <p>PCA - Principal Component Analysis \u21a9\u21a9</p> </li> <li> <p>Principal Component Analysis (PCA) from Scratch \u21a9</p> </li> <li> <p>t-SNE - t-distributed Stochastic Neighbor Embedding \u21a9</p> </li> <li> <p>SMOTE - Synthetic Minority Over-sampling Technique \u21a9</p> </li> <li> <p>Focal Loss for Dense Object Detection \u21a9</p> </li> <li> <p>Word Embeddings - Word2Vec, GloVe, FastText \u21a9</p> </li> </ol>"},{"location":"classes/preprocessing/normalization-vs-standardization/","title":"Normalization vs standardization","text":""},{"location":"classes/preprocessing/normalization-vs-standardization/#normalization-vs-standardization-in-neural-network-preprocessing","title":"Normalization vs. Standardization in Neural Network Preprocessing","text":"<p>In data preprocessing for neural networks, both normalization and standardization are feature scaling techniques used to handle features with different scales, improve model convergence, stabilize gradients during training, and prevent features with larger ranges from dominating the learning process. These methods are particularly important for optimization algorithms like gradient descent, which neural networks rely on.</p> <ul> <li> <p>Normalization (Min-Max Scaling): This scales the data to a fixed range, typically [0, 1] or [-1, 1], using the formula: \\( \\displaystyle x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\). It preserves the original data distribution but is sensitive to outliers, as extreme values can compress the rest of the data into a narrow interval.</p> </li> <li> <p>Standardization (Z-Score Scaling): This transforms the data to have a mean of 0 and a standard deviation of 1, using the formula: \\( \\displaystyle z = \\frac{x - \\mu}{\\sigma} \\), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. It centers the data and is more robust to outliers, but it does not bound the values to a specific range.</p> </li> </ul> <p>There is no universal \"better\" method; the choice depends on the data characteristics, the neural network architecture, and empirical testing. If the dataset is small, it's often worth experimenting with both to see which yields better performance. Below, I'll outline guidelines for when each is appropriate, with specific situations or cases, especially in the context of neural networks.</p> <p>When to Use Normalization</p> <p>Normalization is preferred when the data needs to be bounded within a specific range, the distribution is unknown or non-Gaussian, and there are no significant outliers. It helps avoid numeric overflow in neural networks, speeds up learning, and works well with activation functions that expect inputs in a constrained range. Key situations include:</p> <ul> <li> <p>Bounded Data or Activation Functions Sensitive to Range: Use normalization for neural networks with sigmoid or tanh activations, as these functions perform better with inputs scaled to [0, 1] or [-1, 1] to prevent saturation (where gradients become near zero). For example, in image classification tasks with convolutional neural networks (CNNs), pixel values (typically 0-255) are often normalized to [0, 1] to ensure consistent scaling and faster convergence.</p> </li> <li> <p>Features with Known Min/Max Bounds and No Outliers: When the data has clear minimum and maximum values (e.g., sensor readings bounded between fixed limits), normalization prevents larger-scale features from dominating. A case is processing demographic data like age (e.g., 0-100) in a feedforward neural network for prediction tasks, where scaling to [0, 1] maintains proportionality without assuming a normal distribution.</p> </li> <li> <p>General Speed Improvements in Training: In scenarios where neural networks handle features like age and weight, normalization to [0, 1] can accelerate training and testing by keeping inputs small and consistent, reducing the risk of overflow.</p> </li> </ul> <p>When to Use Standardization</p> <p>Standardization is suitable when the data approximates a Gaussian (normal) distribution, outliers are present, or the model benefits from centered data with unit variance. It helps prevent gradient saturation in neural networks, improves numerical stability, and is often the default choice for many algorithms. Specific cases include:</p> <ul> <li> <p>Data with Outliers or Unknown Distribution: Standardization is more robust to outliers, as it doesn't compress values into a fixed range like normalization does. For instance, in financial datasets for stock price prediction using recurrent neural networks (RNNs), where extreme values (e.g., market crashes) are common, standardization preserves the relative importance of outliers without skewing the scale.</p> </li> <li> <p>Gaussian-Like Data or Convergence-Focused Models: When features follow a bell-curve distribution (verifiable by plotting), standardization aligns with assumptions in techniques like batch normalization in deep neural networks. An example is sensor data analysis in IoT applications with neural networks, where standardization ensures faster gradient descent convergence by centering the data.</p> </li> <li> <p>Standard Practice for Neural Networks: As recommended in foundational work like Yann LeCun's efficient backpropagation paper, scaling to mean 0 and variance 1 is a go-to method to avoid saturating hidden units and handle numerical issues in training. This is common in large-scale datasets for tasks like natural language processing with transformers.</p> </li> </ul> <p>In practice, for neural networks, standardization is often preferred as a starting point due to its robustness, but normalization shines in bounded, outlier-free scenarios. Always apply scaling after splitting data into train/test sets to avoid data leakage, and use libraries like scikit-learn's <code>MinMaxScaler</code> or <code>StandardScaler</code> for implementation.</p>"},{"location":"classes/pyspark/","title":"11. PySpark","text":"<p>Spark is a powerful distributed computing framework widely used for big data processing and analytics. Spark is an open-source unified analytics engine for large-scale data processing, with built-in modules for streaming, SQL, machine learning, and graph processing. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.</p> <p>In this section, we will explore how to set up and run a Spark cluster using Docker Compose. Adapted from <sup>1</sup>.</p>"},{"location":"classes/pyspark/#spark-cluster-on-docker","title":"Spark Cluster on Docker","text":"<p>The Docker setup for Spark includes a directory structure that organizes configuration files, data, scripts, and logs. Below is an overview of the directory structure and its contents:</p> <pre><code>\ud83d\udcc1 docker/\n\u251c\u2500\u2500 \ud83d\udcc1 config/ # (1)\n\u2502   \u251c\u2500\u2500  log4j2.properties # (2)\n\u2502   \u2514\u2500\u2500  spark-defaults.conf # (3)\n\u251c\u2500\u2500 \ud83d\udcc1 data/ # (4)\n\u251c\u2500\u2500 \ud83d\udcc1 scripts/ # (5)\n\u251c\u2500\u2500 \ud83d\udcc1 logs/ # (6)\n\u251c\u2500\u2500  compose.yaml #(7)\n\u2514\u2500\u2500  .gitignore\n</code></pre> <ol> <li>Contains configuration files for Spark.</li> <li>Configuration file for Spark logging.</li> <li>Default configuration settings for Spark.</li> <li>Directory to store datasets used in Spark examples.</li> <li>Contains helper scripts to start and manage the Spark environment.</li> <li>Directory where Spark logs will be stored.</li> <li>Docker Compose file to configure and start Spark services.</li> </ol> Compose fileLog4j ConfigurationSpark Defaults Configuration <p>To set up a Spark cluster using Docker Compose, use the following configuration (compose.yaml):</p> compose.yaml<pre><code>name: spark-cluster\n\nservices:\n\n  spark-master:\n    build:\n      dockerfile_inline: |\n        FROM spark:latest\n        USER root\n        RUN apt update &amp;&amp; apt install -y bash curl wget &amp;&amp; \\\n            python3 -m pip install --upgrade pip &amp;&amp; \\\n            pip3 install numpy pandas matplotlib seaborn scikit-learn requests &amp;&amp; \\\n            rm -rf /var/lib/apt/lists/*\n        USER spark\n        ENV SPARK_HOME=/opt/spark\n        WORKDIR /opt/spark\n    container_name: spark-master\n    hostname: spark-master\n    environment:\n      - SPARK_MODE=master\n      - SPARK_NO_DAEMONIZE=true\n      - PATH=/opt/spark/bin:$PATH\n    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master\n    ports:\n      - 8080:8080  # Spark Master Web UI\n      - \"7077:7077\"  # Spark Master port\n    volumes:\n      - ./data:/opt/spark/data\n      - ./apps:/opt/spark/apps\n      - ./scripts:/opt/spark/scripts\n      - ./config:/opt/spark/conf\n      - ./logs:/opt/spark/logs\n    networks:\n      - spark-network\n\n  spark-worker:\n    image: spark:latest\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_WORKER_CORES=1\n      - SPARK_WORKER_MEMORY=1g\n      - SPARK_NO_DAEMONIZE=true\n    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077\n    volumes:\n      - ./config:/opt/spark/conf\n      - ./logs:/opt/spark/logs\n    deploy:\n      replicas: 3\n    depends_on:\n      - spark-master\n    networks:\n      - spark-network\n\n  spark-history:\n    image: spark:latest\n    container_name: spark-history\n    hostname: spark-history\n    environment:\n      - SPARK_NO_DAEMONIZE=true\n    command: /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer\n    ports:\n      - \"18080:18080\"  # Spark History Server Web UI\n    volumes:\n      - ./config:/opt/spark/conf\n      - ./logs:/opt/spark/logs\n    depends_on:\n      - spark-master\n    networks:\n      - spark-network\n\nnetworks:\n  spark-network:\n    driver: bridge\n</code></pre> <p>The <code>log4j2.properties</code> file configures logging for Spark. Below is a sample configuration (log4j2.properties):</p> log4j2.properties<pre><code># Set root logger level to ERROR and attach to console appender\nlog4j.rootCategory=ERROR, console\n\n# Console appender configuration\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# Suppress INFO logs for specific Spark components (optional)\nlog4j.logger.org.apache.spark=WARN\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.akka=WARN\n</code></pre> <p>The <code>spark-defaults.conf</code> file sets default configurations for Spark. Below is a sample configuration (spark-defaults.conf):</p> spark-defaults.conf<pre><code>spark.eventLog.enabled           true\nspark.eventLog.dir               file:/opt/spark/logs\nspark.history.fs.logDirectory    file:/opt/spark/logs\n</code></pre>"},{"location":"classes/pyspark/#starting","title":"Starting","text":"<p>To start the Spark cluster, navigate to the <code>docker/</code> directory and run the following command:</p> Start Spark Cluster<pre><code>docker compose up -d --build\n</code></pre> <p>The configuration defines a Spark master, three worker nodes, and a history server, along with necessary environment variables and volume mounts for configuration files and data.</p> <pre><code>graph LR\n    user@{ icon: \":octicons-person-24:\", form: \"square\", label: \"User\", pos: \"t\", h: 60 }\n    subgraph Spark Cluster\n        direction LR\n        master[Master Node]\n        worker1([Worker Node 1])\n        worker2([Worker Node 2])\n        worker3([Worker Node 3])\n        history[(History Server)]\n    end\n    user --&gt;|Submits Jobs| master\n    user --&gt;|Accesses UI| history\n    master --&gt; worker1\n    master --&gt; worker2\n    master --&gt; worker3\n    master --&gt; history</code></pre>"},{"location":"classes/pyspark/#accessing","title":"Accessing","text":"<p>Once the Spark cluster is up and running, you can access the Spark Master UI and History Server UI through your web browser:</p> <ul> <li>Spark Master UI: http://localhost:8080</li> <li>Spark History Server UI: http://localhost:18080</li> </ul> <p>These interfaces allow you to monitor the status of your Spark cluster, view running jobs, and analyze job history.</p>"},{"location":"classes/pyspark/#setup","title":"Setup","text":"<p>The whole sample with compose file, directory structure, and configuration could be found at spark-docker.zip (~193 MB).</p>"},{"location":"classes/pyspark/#exercise","title":"Exercise","text":"<p>Entrega</p> <p> 04.dez 23:59</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>To further explore Spark and its capabilities, consider following this comprehensive tutorial that guides you through the basics of Pyspark and how to get started with it:</p> <p>Pyspark Tutorial: Getting Started with Pyspark (starting from Step 1: Creating a SparkSession)</p> <p>Example</p> <p>The <code>ml.py</code> script in the <code>scripts/</code> directory provides a basic example of how to use Pyspark for machine learning tasks and this works with the same dataset as in the tutorial.</p> <p>This tutorial covers essential topics such as setting up a Spark environment, loading and processing data, and performing basic data analysis using Pyspark. The dataset used in the tutorial can be found in the <code>data/</code> directory of the Docker setup. Also, feel free to experiment with the provided scripts in the <code>scripts/</code> directory to deepen your understanding of Spark's functionalities.</p> <p>The exercise will help you gain hands-on experience with Spark and enhance your data processing skills using Pyspark.</p> <p>Executing Scripts Inside the Spark Master Container</p> <p>To run a script inside the Spark master container, use the following command:</p> <pre><code>docker exec -it spark-master /opt/spark/bin/spark-submit /opt/spark/scripts/&lt;your_script.py&gt;\n</code></pre> <p>Replace <code>&lt;your_script.py&gt;</code> with the name of the script you want to execute. This command allows you to submit Spark jobs directly from within the master node of your Spark cluster.</p> <p>The results could be accessed through the folder <code>data/</code> mounted inside the container at <code>/opt/spark/data/</code>.</p> <p>To plot graphs or visualize results, you might need to plot them in files (e.g., PNG) and then save them to the mounted <code>data/</code> directory for access outside the container.</p> <p>Alternative: You can also run the scripts from inside the Spark master container.</p> <p></p>docker exec -it spark-master bashspark@spark-master:/opt/spark$ spark-submit ./scripts/ml.pyWARNING: Using incubator modules: jdk.incubator.vector...<p></p> <p>Adaptation is needed</p> <p>Pay attention that running Spark in Docker may require adapting the code to work correctly within the containerized environment. This includes ensuring that file paths, environment variables, and dependencies are correctly set up to match the Docker configuration.</p>"},{"location":"classes/pyspark/#delivering","title":"Delivering","text":"<p>The exercise is considered complete when you have successfully set up the Spark cluster using Docker Compose, accessed the Spark UIs, executed at least one Pyspark script from the <code>scripts/</code> directory, and run the whole tutorial. Additionally, you should be able to analyze the results of your Spark jobs and visualize any outputs as needed.</p> <p>Also, this exercise can be delivered by sharing a brief report or summary of your experience, including any challenges faced and how you overcame them while working with Spark in a Dockerized environment. The report can include screenshots of the Spark UIs, code snippets from the executed scripts, and any insights gained from the data analysis performed using Pyspark. Add the report to your learning portfolio (github Pages) for future reference.</p>"},{"location":"classes/pyspark/#criteria","title":"Criteria","text":"Points Criteria 2 Spark cluster setup using Docker Compose is incomplete or not functional. 2 Spark cluster is set up, but access to Spark UIs is not demonstrated. 1 Executed at least one Pyspark example script. 3 Successfully executed the all tutorial. With plots and visualizations. 2 Provided a comprehensive report on the experience. <ol> <li> <p>Running Spark using Docker Compose \u21a9</p> </li> </ol>"},{"location":"classes/random_forest/","title":"7. Random Forest","text":"<p>Random forests are a popular ensemble learning algorithm in machine learning, primarily used for classification and regression tasks. Introduced by Leo Breiman in 2001, they build upon decision trees by combining multiple trees into a \"forest\" to improve accuracy, reduce overfitting, and enhance generalization. The key idea is to create diversity among the trees through randomness, which helps in averaging out errors from individual trees.</p> <p></p> <p>Random Forest Algorithm. Source: GeeksforGeeks</p>"},{"location":"classes/random_forest/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Ensemble Method: Random forests use bagging (bootstrap aggregating), where each decision tree is trained on a random subset of the training data (with replacement). This reduces variance.</li> <li>Random Feature Selection: At each node split in a tree, only a random subset of features is considered, which decorrelates the trees and further reduces overfitting.</li> <li>Prediction:<ul> <li>For classification: The final output is the majority vote (mode) from all trees.</li> <li>For regression: The final output is the average (mean) of predictions from all trees.</li> </ul> </li> <li>Advantages: Robust to noise, handles missing values well, provides feature importance scores, and works well with high-dimensional data.</li> <li>Disadvantages: Can be computationally intensive, less interpretable than single decision trees, and may require tuning hyperparameters like number of trees (<code>n_estimators</code>), maximum depth (<code>max_depth</code>), and number of features per split (<code>max_features</code>).</li> <li>Applications: Used in finance (credit scoring), healthcare (disease prediction), e-commerce (recommendation systems), and more.</li> </ul> <p>Random forests also offer out-of-bag (OOB) error estimation, where each tree is evaluated on the data not included in its bootstrap sample, providing a built-in cross-validation metric.</p>"},{"location":"classes/random_forest/#formulas-and-mathematical-foundations","title":"Formulas and Mathematical Foundations","text":"<p>Random forests build on decision trees, where each tree minimizes impurity (e.g., Gini or entropy for classification, MSE for regression) at splits.</p> <ol> <li> <p>Bootstrap Sampling:</p> <ul> <li>Given a dataset \\( D \\) with \\( N \\) samples, for each tree \\( t = 1 \\) to \\( T \\):<ul> <li>Sample \\( D_t \\) (bootstrap dataset) with replacement from \\( D \\), typically of size \\( N \\).</li> </ul> </li> </ul> </li> <li> <p>Feature Subset Selection:</p> <ul> <li>At each node, select a random subset of \\( m \\) features from total \\( p \\) features (often \\( m = \\sqrt{p} \\) for classification or \\( m = p/3 \\) for regression).</li> </ul> </li> <li> <p>Tree Construction:</p> <ul> <li> <p>For classification, impurity measures:</p> <ul> <li> <p>Gini Impurity</p> \\[ G = \\sum_{k=1}^K p_k (1 - p_k) \\] <p>where \\( p_k \\) is the proportion of class \\( k \\) in the node.</p> </li> <li> <p>Entropy</p> \\[ E = -\\sum_{k=1}^K p_k \\log_2(p_k) \\] <p>where \\( p_k \\) is the proportion of class \\( k \\) in the node.</p> </li> </ul> </li> <li> <p>Split to minimize weighted impurity:</p> \\[ \\Delta I = I(parent) - \\sum_{child} \\frac{N_{child}}{N_{parent}} I(child) \\] </li> </ul> </li> <li> <p>Ensemble Prediction:</p> <ul> <li> <p>For classification:</p> \\[ \\hat{y} = \\arg\\max_k \\left( \\frac{1}{T} \\sum_{t=1}^T I(\\hat{y}_t = k) \\right) \\] <p>where \\( I \\) is the indicator function, and \\( \\hat{y}_t \\) is the prediction from tree \\( t \\).</p> </li> <li> <p>For regression:</p> \\[ \\hat{y} = \\frac{1}{T} \\sum_{t=1}^T \\hat{y}_t \\] </li> </ul> </li> <li> <p>Out-of-Bag (OOB) Error:</p> <ul> <li>For each sample \\( i \\), predict using only trees where \\( i \\) was not in the bootstrap sample.</li> <li>OOB error = average error over all samples (e.g., misclassification rate or MSE).</li> </ul> </li> <li> <p>Feature Importance:</p> <ul> <li>Often measured by mean decrease in impurity (MDI): Sum of \\( \\Delta I \\) across all splits using that feature, averaged over trees.</li> <li>Or permutation importance: Decrease in model score when feature values are randomly shuffled.</li> </ul> </li> </ol> <p>These formulas ensure the forest's bias-variance tradeoff is optimized, with low bias from deep trees and low variance from averaging.</p>"},{"location":"classes/random_forest/#from-scratch","title":"From Scratch","text":"<p>Implementing a random forest from scratch requires building a basic decision tree first, then ensembling them. Below is a simplified version for classification using only standard Python (no external libraries like NumPy for arrays\u2014using lists instead). This is for educational purposes; real implementations use optimized libraries.</p> <p>We'll assume a binary classification problem with features as lists of lists and labels as a list (0 or 1). It uses Gini impurity and random subsets.</p> <pre><code>import random\nfrom collections import Counter\n\n# Helper: Calculate Gini impurity\ndef gini_impurity(y):\n    if not y:\n        return 0\n    counts = Counter(y)\n    impurity = 1\n    for count in counts.values():\n        prob = count / len(y)\n        impurity -= prob ** 2\n    return impurity\n\n# Helper: Split dataset based on feature index and value\ndef split_dataset(X, y, feature_idx, value):\n    left_X, left_y, right_X, right_y = [], [], [], []\n    for i in range(len(X)):\n        if X[i][feature_idx] &lt;= value:\n            left_X.append(X[i])\n            left_y.append(y[i])\n        else:\n            right_X.append(X[i])\n            right_y.append(y[i])\n    return left_X, left_y, right_X, right_y\n\n# Decision Tree Node\nclass Node:\n    def __init__(self, feature_idx=None, value=None, left=None, right=None, label=None):\n        self.feature_idx = feature_idx\n        self.value = value\n        self.left = left\n        self.right = right\n        self.label = label  # Leaf node label\n\n# Build a single decision tree (recursive)\ndef build_tree(X, y, max_depth, min_samples_split, max_features):\n    if len(y) &lt; min_samples_split or max_depth == 0:\n        return Node(label=Counter(y).most_common(1)[0][0])\n\n    n_features = len(X[0])\n    features = random.sample(range(n_features), max_features)  # Random subset\n\n    best_gini = float('inf')\n    best_feature_idx, best_value = None, None\n    best_left_X, best_left_y, best_right_X, best_right_y = None, None, None, None\n\n    for feature_idx in features:\n        values = sorted(set(row[feature_idx] for row in X))\n        for value in values:\n            left_X, left_y, right_X, right_y = split_dataset(X, y, feature_idx, value)\n            if not left_y or not right_y:\n                continue\n            p_left = len(left_y) / len(y)\n            gini = p_left * gini_impurity(left_y) + (1 - p_left) * gini_impurity(right_y)\n            if gini &lt; best_gini:\n                best_gini = gini\n                best_feature_idx = feature_idx\n                best_value = value\n                best_left_X, best_left_y = left_X, left_y\n                best_right_X, best_right_y = right_X, right_y\n\n    if best_gini == float('inf'):\n        return Node(label=Counter(y).most_common(1)[0][0])\n\n    left = build_tree(best_left_X, best_left_y, max_depth - 1, min_samples_split, max_features)\n    right = build_tree(best_right_X, best_right_y, max_depth - 1, min_samples_split, max_features)\n    return Node(best_feature_idx, best_value, left, right)\n\n# Predict with a single tree\ndef predict_tree(node, x):\n    if node.label is not None:\n        return node.label\n    if x[node.feature_idx] &lt;= node.value:\n        return predict_tree(node.left, x)\n    else:\n        return predict_tree(node.right, x)\n\n# Random Forest class\nclass RandomForest:\n    def __init__(self, n_estimators=10, max_depth=5, min_samples_split=2, max_features='sqrt'):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.max_features = max_features\n        self.trees = []\n\n    def fit(self, X, y):\n        n_samples = len(X)\n        n_features = len(X[0])\n        max_features = int(n_features ** 0.5) if self.max_features == 'sqrt' else self.max_features\n\n        for _ in range(self.n_estimators):\n            # Bootstrap sample\n            bootstrap_idx = [random.randint(0, n_samples - 1) for _ in range(n_samples)]\n            X_boot = [X[i] for i in bootstrap_idx]\n            y_boot = [y[i] for i in bootstrap_idx]\n            tree = build_tree(X_boot, y_boot, self.max_depth, self.min_samples_split, max_features)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        predictions = []\n        for x in X:\n            tree_preds = [predict_tree(tree, x) for tree in self.trees]\n            predictions.append(Counter(tree_preds).most_common(1)[0][0])\n        return predictions\n\n# Example usage\n# X = [[0, 0], [1, 1], [1, 0], [0, 1]]  # Features\n# y = [0, 1, 0, 1]  # Labels\n# rf = RandomForest(n_estimators=3, max_depth=2)\n# rf.fit(X, y)\n# print(rf.predict([[0.5, 0.5]]))  # Output: [1] or similar\n</code></pre> <p>This implementation is basic and not optimized (e.g., no handling for continuous features beyond simple splits, no OOB). For real use, add error handling and optimizations.</p>"},{"location":"classes/random_forest/#with-library","title":"With Library","text":"<p>For practical applications, use scikit-learn's <code>RandomForestClassifier</code> or <code>RandomForestRegressor</code>. It handles everything efficiently, including parallel tree building.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load example dataset (Iris for classification)\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the model\nrf = RandomForestClassifier(n_estimators=100,  # Number of trees\n                            max_depth=5,       # Max depth of trees\n                            max_features='sqrt',  # Features per split\n                            random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and evaluate\npredictions = rf.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n\n# Feature importances\nprint(f\"Feature Importances: {rf.feature_importances_}\")\n</code></pre> <p>This uses the Iris dataset for demonstration. You can replace it with your data. scikit-learn handles bootstrapping, randomness, and predictions automatically. For regression, swap to <code>RandomForestRegressor</code> and use metrics like MSE.</p>"},{"location":"classes/random_forest/#additional","title":"Additional","text":""},{"location":"classes/random_forest/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p> 28.out 23:59</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize o algoritmo de Random Forest para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio Random Forest. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 20 2 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 10 3 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 20 4 Treinamento do Modelo Implementa\u00e7\u00e3o do modelo Random Forest. 10 5 Avalia\u00e7\u00e3o do Modelo Avalia\u00e7\u00e3o do desempenho do modelo utilizando m\u00e9tricas apropriadas. 20 6 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. Obrigat\u00f3rio: uso do template-projeto-integrador, individual. 20 <ol> <li> <p>Random Forest Algorithm in Machine Learning \u21a9</p> </li> <li> <p>Random Forest - Simple Explanation \u21a9</p> </li> </ol>"},{"location":"classes/svm/","title":"12. SVM","text":"<p>Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification tasks, though it can also handle regression. At its core, SVM aims to find the optimal hyperplane that separates data points of different classes with the maximum possible margin. The margin is the distance between the hyperplane and the nearest data points from each class, known as support vectors. By maximizing this margin, SVM promotes better generalization to unseen data.</p> <p>SVM can be \"hard-margin\" (no misclassifications allowed, assuming perfect separability) or \"soft-margin\" (allows some misclassifications via a regularization parameter C to handle noise or overlaps).</p> <p></p> <p>For linearly separable data, the hyperplane is a straight line (in 2D) or a plane (in higher dimensions). However, real-world data is often not linearly separable. This is where the \"kernel trick\" comes in\u2014it implicitly maps the data into a higher-dimensional space where it becomes linearly separable, without explicitly computing the transformation. Common kernels include linear, polynomial, radial basis function (RBF), and sigmoid.</p>"},{"location":"classes/svm/#kernel-trick","title":"Kernel Trick","text":"<p>The kernel trick allows SVM to operate in a high-dimensional space without explicitly computing the coordinates of the data in that space. Instead, it computes the inner products between all pairs of data points in the original space using a kernel function. This enables SVM to find non-linear decision boundaries efficiently.</p> 1D to 2D2D to 3D 2025-11-14T11:31:43.493996 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p></p> <p>Mapping from 2D to 3D space. The kernel function allows SVM to find a linear separator in the higher-dimensional space, which corresponds to a non-linear boundary in the original space. Souce: <sup>2</sup>.</p> <p>Common kernel functions include:</p> Kernel Equation Linear \\( K(x,y) = x \\cdot y \\) Sigmoid \\( K(x,y) = \\tanh(ax \\cdot y + b) \\) Polynomial \\( K(x,y) = (1 + x \\cdot y)^d \\) Radial Basis Function (RBF) \\( K(x,y) = e^{-\\gamma \\|x-y\\|^2} \\) ResultCode 2025-11-14T11:31:43.819813 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>from sklearn.datasets import load_breast_cancer\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom io import StringIO\n\ncancer = load_breast_cancer()\nX = cancer.data[:, :2]\ny = cancer.target\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 6))\n\nkernels = {\n    'linear': ax1,\n    'sigmoid': ax2,\n    'poly': ax3,\n    'rbf': ax4\n}\n\nfor k, ax in kernels.items():\n    svm = SVC(kernel=k, C=1)\n    svm.fit(X, y)\n\n    DecisionBoundaryDisplay.from_estimator(\n        svm,\n        X,\n        response_method=\"predict\",\n        alpha=0.8,\n        cmap=\"Pastel1\",\n        ax=ax\n    )\n    ax.scatter(\n        X[:, 0], X[:, 1], \n        c=y, \n        s=20, edgecolors=\"k\"\n    )\n    ax.set_title(k)\n    ax.set_xticks([]) \n    ax.set_yticks([])\n\n# Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n\nplt.close()\n</code></pre>"},{"location":"classes/svm/#applications","title":"Applications","text":"<p>SVM is versatile and used in:</p> <ul> <li>Image classification (e.g., handwritten digit recognition).</li> <li>Text categorization (e.g., spam detection).</li> <li>Bioinformatics (e.g., protein classification).</li> <li>Finance (e.g., stock trend prediction).</li> <li>Face detection in computer vision.</li> </ul>"},{"location":"classes/svm/#pros-and-cons","title":"Pros and Cons","text":"<ul> <li> <p>Pros</p> <ul> <li>Effective in high-dimensional spaces.</li> <li>Robust to overfitting, especially with appropriate kernels and C.</li> <li>Memory efficient, as it only relies on support vectors.</li> <li>Versatile with different kernels for non-linear problems.</li> </ul> </li> <li> <p>Cons</p> <ul> <li>Computationally intensive for large datasets (O(n^2) for kernel matrix).</li> <li>Sensitive to choice of kernel and parameters (requires tuning).</li> <li>Not probabilistic (doesn't output probabilities directly; needs extensions like Platt scaling)<sup>8</sup>.</li> <li>Poor performance on noisy data without proper regularization.</li> </ul> </li> </ul>"},{"location":"classes/svm/#terminology","title":"Terminology","text":"<ul> <li>Hyperplane: The decision boundary that separates classes. In 2D, it's a line; in 3D, a plane.</li> <li>Support Vectors: The data points closest to the hyperplane that influence its position and orientation. Only these points matter for the model.</li> <li>Margin: The perpendicular distance from the hyperplane to the support vectors. SVM maximizes this for robustness.<ul> <li>Hard Margin: No misclassifications allowed; assumes perfect separability.</li> <li>Soft Margin: Allows some misclassifications via slack variables (\u03be_i) to handle noise/overlap.</li> </ul> </li> <li>Kernel Trick: A method to compute inner products in a high-dimensional space without explicitly mapping data points, enabling non-linear decision boundaries.</li> <li> <p>Decision Function: The function used to classify new data points:</p> \\[ f(x) = \\sum \\alpha_i y_i K(x_i, x) + b) &gt; 0 \\] </li> <li> <p>Kernel Function: A function that computes the similarity between data points in a transformed feature space.</p> </li> <li>Lagrange Multipliers (\u03b1): Variables used in the dual formulation to solve the optimization problem.</li> <li>Regularization Parameter (C): Controls the trade-off between maximizing the margin and minimizing classification errors in soft-margin SVM.</li> <li>Bias (b): The offset term in the decision function \\( f(x) = w \\cdot x + b &gt; 0 \\).</li> <li>Dual Problem: A reformulation of the primal optimization problem, which is easier to solve, especially with kernels.</li> </ul>"},{"location":"classes/svm/#additional","title":"Additional","text":"Support Vector Machines: All you need to know!Support Vector Machines | ML-005 Lecture 12 | Stanford University | Andrew NgSupport Vector Machines Part 1 (of 3): Main Ideas!!!"},{"location":"classes/svm/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p> 04.dez 23:59</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize o algoritmo de SVM para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio SVM. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 20 2 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 10 3 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 20 4 Treinamento do Modelo Implementa\u00e7\u00e3o do modelo SVM. 10 5 Avalia\u00e7\u00e3o do Modelo Avalia\u00e7\u00e3o do desempenho do modelo utilizando m\u00e9tricas apropriadas. 20 6 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. Obrigat\u00f3rio: uso do template-projeto-integrador, individual. 20 <ol> <li> <p>The Nature of Statistical Learning Theory, Vapnik, 1999.\u00a0\u21a9</p> </li> <li> <p>Support Vector Machines: A Simple Explanation \u21a9</p> </li> <li> <p>Support Vector Machines: A Guide for Beginners - QuantStart \u21a9</p> </li> <li> <p>Support Vector Machine (SVM) Algorithm - GeeksforGeeks \u21a9</p> </li> <li> <p>Scikit-learn SVM Tutorial with Python - DataCamp \u21a9</p> </li> <li> <p>Tutorial on Support Vector Machine (SVM) \u21a9</p> </li> <li> <p>Implementing SVM from Scratch in Python - GeeksforGeeks \u21a9</p> </li> <li> <p>Multi-class classification using Support Vector Machines (SVM) \u21a9</p> </li> <li> <p>Scikit-learn - Support Vector Machines \u21a9</p> </li> </ol>"},{"location":"versions/2025.2/","title":"2025.2","text":""},{"location":"versions/2025.2/#instructor","title":"Instructor","text":"<p> [  : Humberto Sandmann] </p>"},{"location":"versions/2025.2/#students","title":"Students","text":"Name Project ALEXANDRE MARTINELLI alexandremartinelli11.github.io ANA CAROLINA ALBI PEREIRA ANA CAROLINA FRANK PEZZINI DE MENEZES bligui.github.io ANDR\u00c9 HENRIQUE PACHECO ALVES ARTHUR BORBA FERREIRA ARTUR NAPOLES DE OLIVEIRA BIANCA CRISTINE FAGUNDES DE ARA\u00daJO BRUNO DE ASSIS PEREIRA assyss.github.io EDUARDO GUL D'\u00c1VILA eduardogd09.github.io ENRICO COMASSETTO DI GIOIA enricodigioia.github.io ENZO DE MARCHI MALAGOLI enzomalagoli.github.io ENZO DE MORAES GODOY FERNANDO ADAMI HASELOFF PIRES SABELLA GABRIEL CARDOSO CAMPOS RODRIGUES GABRIEL FUZITA CHAVES GUILHERME CARVALHO DOS REIS GUILHERME ORLANDI DE OLIVEIRA GUSTAVO DUTRA TELLES snowdutra.github.io HENRIQUE AGONDI LEITE HUGO COSCELLI FERRAZ z-hugo-ferraz.github.io ISABELLA FRAN\u00c7A VALLAND JOAO PEDRO SANTOS HELBEL JOS\u00c9 LONGO NETO jose-longo-a.github.io JULIA AKEMI MULLIS akemi-m.github.io LUIS FELIPE GALINA DEGASPARI LUIZ FELIPE PIMENTA BERRETTINI LUIZ FERNANDO PAZDZIORA COSTA LUZIVANIA JESUS BONFIM bonfim1.github.io MARCELA PAV\u00c3O DE MARTINI marcelademartini.github.io M\u00c1RCIO ALEXANDRONI DA SILVA FILHO MARIA GABRIELA VIEIRA DOS SANTOS mgabriel4.github.io MARIA LUIZA OLIVEIRA DE SOUZA MARTIM ROZANCZYK PONZIO MATEUS CARNEVALE COLMEAL colmeal.github.io PABLO DIMITROF DE SIQUEIRA pablodimitrof.github.io PEDRO ALMEIDA MARICATE SANTOS pedromaricate.github.io RAFAEL ARKCHIMOR LUCENA rafaarklu.github.io THEO CAMURI GASPAR tigasparzin.github.io YCARO CAMPOVILLA MENDES DE BARROS"},{"location":"versions/2025.2/#schedule","title":"Schedule","text":"<p> [Tue. : 09h30  11h10]   [Fri. : 07h30  09h10] </p>"},{"location":"versions/2025.2/#grade","title":"Grade","text":"2025-11-14T11:31:44.126459 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/"},{"location":"versions/2025.2/projects/1/","title":"Project I","text":"<p>Entrega</p> <p> 30.set 23:59</p> <p> Grupo do Projeto Integrador</p> <p> Entrega do link do projeto via Canvas.</p> <p>O link deve apontar para uma p\u00e1gina p\u00fablica do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o reposit\u00f3rio do projeto.</p> <p>Obrigat\u00f3rio: o uso de mkdocs ou similar para a documenta\u00e7\u00e3o do projeto. Utilize o template: template-projeto-integrador.</p> <p>O Projeto I \u00e9 uma atividade pr\u00e1tica que visa aplicar os conceitos aprendidos nas aulas de Machine Learning. O objetivo \u00e9 desenvolver um modelo de aprendizado de m\u00e1quina utilizando um conjunto de dados real, realizando desde a explora\u00e7\u00e3o dos dados at\u00e9 a avalia\u00e7\u00e3o do modelo.</p> <p>Para este projeto, voc\u00ea pode escolher um conjunto de dados dispon\u00edvel em plataformas como o Kaggle Datasets ou utilizar um conjunto de dados pr\u00f3prio.</p> <p>O projeto pode ser focado em classifica\u00e7\u00e3o ou regress\u00e3o, dependendo do conjunto de dados escolhido. \u00c9 importante que o conjunto de dados tenha caracter\u00edsticas que permitam a aplica\u00e7\u00e3o de diferentes algoritmos de aprendizado de m\u00e1quina, como Decision Tree, K-Nearest Neighbors (KNN) e K-Means.</p> <p>O projeto deve incluir as seguintes etapas:</p> <ol> <li>Explora\u00e7\u00e3o dos Dados: An\u00e1lise inicial do conjunto de dados, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas.</li> <li>Pr\u00e9-processamento: Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o.</li> <li>Divis\u00e3o dos Dados: Separa\u00e7\u00e3o do conjunto de dados em treino e teste.</li> <li>Treinamento do Modelo:<ul> <li>Decision Tree.</li> <li>K-Nearest Neighbors (KNN).</li> <li>K-Means.</li> </ul> </li> <li>Avalia\u00e7\u00e3o dos Modelos: Avalia\u00e7\u00e3o do desempenho dos modelos utilizandos com as m\u00e9tricas apropriadas.</li> <li>Relat\u00f3rio Final: Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias.</li> </ol> <p>Pode ser utilizado bibliotecas como <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p>"},{"location":"versions/2025.2/projects/1/#rubrica-de-avaliacao","title":"Rubrica de Avalia\u00e7\u00e3o","text":"Crit\u00e9rio Descri\u00e7\u00e3o Pontos Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 15 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 20 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 10 Treinamento do Modelo Implementa\u00e7\u00e3o dos modelos Decision Tree, KNN e K-Means. 15 Avalia\u00e7\u00e3o dos Modelos Avalia\u00e7\u00e3o e compara\u00e7\u00e3o do desempenho dos modelos utilizando m\u00e9tricas apropriadas. 30 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. 10"},{"location":"versions/2025.2/projects/2/","title":"Project II","text":"<p>Entrega</p> <p> 05.dez 23:59</p> <p> Grupo do Projeto Integrador</p> <p> Entrega do link do projeto via Canvas.</p> <p>O link deve apontar para uma p\u00e1gina p\u00fablica do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o reposit\u00f3rio do projeto.</p> <p>Obrigat\u00f3rio: o uso de mkdocs ou similar para a documenta\u00e7\u00e3o do projeto. Utilize o template: template-projeto-integrador.</p> <p>O Projeto II busca aplicar Machine Learning no contexto do Projeto Integrador.</p> <p>O objetivo \u00e9 implementar, ao menos 3 algoritmos dos estudados, sobre os dados do projeto, e analisar os resultados. A escolha dos algoritmos deve ser justificada no relat\u00f3rio, considerando a natureza dos dados e os objetivos do projeto.</p> <p>O projeto deve incluir as seguintes etapas:</p> <ol> <li> <p>Explora\u00e7\u00e3o dos Dados: An\u00e1lise inicial do conjunto de dados, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas.</p> <p>Caso n\u00e3o haja um hist\u00f3rico significativo de dados, deve-se criar um conjunto de dados sint\u00e9tico que simule o cen\u00e1rio do projeto.</p> </li> <li> <p>Regress\u00e3o: Prever uma vari\u00e1vel cont\u00ednua (ex.: pre\u00e7o, temperatura, vendas). Esse projeto deve ser focado em regress\u00e3o, ou seja, prever valores num\u00e9ricos cont\u00ednuos.</p> </li> <li> <p>Sele\u00e7\u00e3o dos Modelos: Escolha de pelo menos tr\u00eas algoritmos de Machine Learning para implementar.</p> </li> <li> <p>Avalia\u00e7\u00e3o dos Modelos: Avalia\u00e7\u00e3o do desempenho dos modelos utilizandos com as m\u00e9tricas apropriadas. Compara\u00e7\u00e3o dos resultados.</p> </li> <li> <p>Relat\u00f3rio Final: Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias.</p> </li> </ol> <p>Pode ser utilizado bibliotecas como <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p>"},{"location":"versions/2025.2/projects/2/#rubrica-de-avaliacao","title":"Rubrica de Avalia\u00e7\u00e3o","text":"Crit\u00e9rio Descri\u00e7\u00e3o Pontos Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 10 Gera\u00e7\u00e3o de Dados Sint\u00e9ticos Cria\u00e7\u00e3o de um conjunto de dados sint\u00e9tico que simule o cen\u00e1rio do projeto, caso n\u00e3o haja um hist\u00f3rico significativo de dados. 10 Implementa\u00e7\u00e3o dos Modelos Implementa\u00e7\u00e3o correta dos algoritmos escolhidos, com justificativa adequada. 30 Avalia\u00e7\u00e3o dos Modelos Avalia\u00e7\u00e3o do desempenho dos modelos utilizando as m\u00e9tricas apropriadas e compara\u00e7\u00e3o dos resultados. 30 Relat\u00f3rio Final Documenta\u00e7\u00e3o clara e completa do processo, resultados obtidos e poss\u00edveis melhorias. 20"},{"location":"versions/2025.2/projects/2/#entrega","title":"Entrega","text":"<p>A entrega deve ser feita atrav\u00e9s do Canvas - Projeto de Dados II. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE PROJETO PODE SER FEITO COM O GRUPO DO PROJETO INTEGRADOR.</p>"}]}